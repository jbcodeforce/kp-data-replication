{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This repository includes a set of documents for best practices around data replication between two Kafka clusters. Mirror Maker 2.0 Mirror Maker 2.0 is the new replication feature of Kafka 2.4. It was defined as part of the KIP 382 . General concepts As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note . The figure below illustrates the mirror maker internal components running within Kafka Connect. In distributed mode, Mirror Maker creates the following topics to the target cluster: mm2-configs.source.internal: This topic will store the connector and task configurations. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic will store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical mirror maker configuration is done via property file and defines source and target clusters with their connection properties and the replication flow definitions. Here is a simple example for a local cluster to a target cluster using TLS v1.2 for connection encryption and Sasl authentication protocol to connect to Event Streams. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] But we can also define the samethings with the properties: topics.blacklist . Comma-separated lists are also supported and Java regular expression. Internally MirrorSourceConnector and MirrorCheckpointConnector will create multiple tasks (up to tasks.max property), MirrorHeartbeatConnector creates only one single task. MirrorSourceConnector will have one task per topic-partition to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka connect framework uses the coordinator API, with the assign() API, so there is no consumer group while fetching data from source topic. There is no call to commit() neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern. Requirements to address Environments We propose two approaches to run the 'on-premise' Kafka cluster: Docker compose using vanilla Kafka 2.4 : To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in mirror-maker-2/local-cluster folder. This compose file uses a local docker network called kafkanet . The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version. We are describing how to setup this simple cluster using Docker compose in this article . Kafka 2.4 cluster using Strimzi operator deployed on Openshift For the Event Streams on Cloud cluster, we recommend to create your own using IBM Cloud Account. The product documentation is here . The enviroments can be summarized in the table below: Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks Local cluster to Event Streams on Cloud The goal is to demonstrate the replicate data from local cluster to Event Streams on IBM Cloud running as managed service. The two scenarios and the step by step approach are presented in this note . We have documented the replication from Event Streams as a Service to local cluster in this note with two scenarios depending of the target Kafka cluster (running on OpenShift or on VM / containers). Provisioning Connectors (Mirror Maker 2) This main epic is related to provisioning operation. As a SRE I want to provision and deploy Mirror Maker 2 connector to existing Openshift cluster without exposing password and keys so replication can start working. This will use Kubernetes secrets for configuration parameters. We are describing the MM2 provisioning in this note . As a SRE I want to understand the CLI commands used to assess to assess how the provisioning process can be automated. We do not proof how to automate the deployment, but as all deployments are done with CLI and configuration files we could consider using Ansible for automation. As a SRE I want to understand the server sizing for the Mirror Maker environment so that I can understand how to use leanest resources for minimal needs. We talk about capacity planning in this section and performance tests here . Note that, there is no specific user interface for mirror maker connector. Version to version migration As a SRE, I want to understand how to perform a version to version migration for the Mirror Maker 2 product so that existing streaming replication is not impacted by the upgrade. As a developer I want to deploy configuration updates to modify the topic white or black lists so that newly added topics are replicated. Security As a SRE, I want to understand how client applications authenticate to source and target clusters. As a developer I want to design Mirror Maker 2 based replication solution to support different line of businesses who should not connect to topics and data not related to their business and security scope. Monitoring As a SRE, I want to get Mirror Maker 2 metrics for Prometheus so that it fits in my current metrics processing practices. The explanation to setup Prometheus metris for mirror maker 2.0 is documented here . As a SRE, I want to be able to add new dashboard into Grafana to visualize the Mirror Maker 2 metrics. As a SRE, I want to define rules for alert reporting and configure a Slack channel for alerting. [Removed] As a SRE, I want to get the Mirror Maker 2 logs into our Splunk logging platform. Best Practices As a developer I want to understand how Mirror Maker 2 based replication addresses the record duplication. As a developer I want to design replication solution to minimize the instance of Mirror Maker or being able to scale them if I observe lag into data replication processing. As a developer I want to understand what are the condition for message loss. Performance tests As a developer I want to understand how to measure data latency and lag in data replication. As a SRE I want to understand current thoughput for the replication solution.","title":"Introduction"},{"location":"#introduction","text":"This repository includes a set of documents for best practices around data replication between two Kafka clusters.","title":"Introduction"},{"location":"#mirror-maker-20","text":"Mirror Maker 2.0 is the new replication feature of Kafka 2.4. It was defined as part of the KIP 382 .","title":"Mirror Maker 2.0"},{"location":"#general-concepts","text":"As Mirror maker 2.0 is using kafka Connect framework, we recommend to review our summary in this note . The figure below illustrates the mirror maker internal components running within Kafka Connect. In distributed mode, Mirror Maker creates the following topics to the target cluster: mm2-configs.source.internal: This topic will store the connector and task configurations. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic will store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical mirror maker configuration is done via property file and defines source and target clusters with their connection properties and the replication flow definitions. Here is a simple example for a local cluster to a target cluster using TLS v1.2 for connection encryption and Sasl authentication protocol to connect to Event Streams. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] But we can also define the samethings with the properties: topics.blacklist . Comma-separated lists are also supported and Java regular expression. Internally MirrorSourceConnector and MirrorCheckpointConnector will create multiple tasks (up to tasks.max property), MirrorHeartbeatConnector creates only one single task. MirrorSourceConnector will have one task per topic-partition to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka connect framework uses the coordinator API, with the assign() API, so there is no consumer group while fetching data from source topic. There is no call to commit() neither: the rebalancing occurs only when there is a new topic created that matches the whitelist pattern.","title":"General concepts"},{"location":"#requirements-to-address","text":"","title":"Requirements to address"},{"location":"#environments","text":"We propose two approaches to run the 'on-premise' Kafka cluster: Docker compose using vanilla Kafka 2.4 : To run local cluster we use docker-compose and docker. The docker compose file to start a local 3 Kafka brokers and 2 Zookeepers cluster is in mirror-maker-2/local-cluster folder. This compose file uses a local docker network called kafkanet . The docker image used for Kafka is coming from Strimzi open source project and is for the Kafka 2.4 version. We are describing how to setup this simple cluster using Docker compose in this article . Kafka 2.4 cluster using Strimzi operator deployed on Openshift For the Event Streams on Cloud cluster, we recommend to create your own using IBM Cloud Account. The product documentation is here . The enviroments can be summarized in the table below: Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks","title":"Environments"},{"location":"#local-cluster-to-event-streams-on-cloud","text":"The goal is to demonstrate the replicate data from local cluster to Event Streams on IBM Cloud running as managed service. The two scenarios and the step by step approach are presented in this note . We have documented the replication from Event Streams as a Service to local cluster in this note with two scenarios depending of the target Kafka cluster (running on OpenShift or on VM / containers).","title":"Local cluster to Event Streams on Cloud"},{"location":"#provisioning-connectors-mirror-maker-2","text":"This main epic is related to provisioning operation. As a SRE I want to provision and deploy Mirror Maker 2 connector to existing Openshift cluster without exposing password and keys so replication can start working. This will use Kubernetes secrets for configuration parameters. We are describing the MM2 provisioning in this note . As a SRE I want to understand the CLI commands used to assess to assess how the provisioning process can be automated. We do not proof how to automate the deployment, but as all deployments are done with CLI and configuration files we could consider using Ansible for automation. As a SRE I want to understand the server sizing for the Mirror Maker environment so that I can understand how to use leanest resources for minimal needs. We talk about capacity planning in this section and performance tests here . Note that, there is no specific user interface for mirror maker connector.","title":"Provisioning Connectors (Mirror Maker 2)"},{"location":"#version-to-version-migration","text":"As a SRE, I want to understand how to perform a version to version migration for the Mirror Maker 2 product so that existing streaming replication is not impacted by the upgrade. As a developer I want to deploy configuration updates to modify the topic white or black lists so that newly added topics are replicated.","title":"Version to version migration"},{"location":"#security","text":"As a SRE, I want to understand how client applications authenticate to source and target clusters. As a developer I want to design Mirror Maker 2 based replication solution to support different line of businesses who should not connect to topics and data not related to their business and security scope.","title":"Security"},{"location":"#monitoring","text":"As a SRE, I want to get Mirror Maker 2 metrics for Prometheus so that it fits in my current metrics processing practices. The explanation to setup Prometheus metris for mirror maker 2.0 is documented here . As a SRE, I want to be able to add new dashboard into Grafana to visualize the Mirror Maker 2 metrics. As a SRE, I want to define rules for alert reporting and configure a Slack channel for alerting. [Removed] As a SRE, I want to get the Mirror Maker 2 logs into our Splunk logging platform.","title":"Monitoring"},{"location":"#best-practices","text":"As a developer I want to understand how Mirror Maker 2 based replication addresses the record duplication. As a developer I want to design replication solution to minimize the instance of Mirror Maker or being able to scale them if I observe lag into data replication processing. As a developer I want to understand what are the condition for message loss.","title":"Best Practices"},{"location":"#performance-tests","text":"As a developer I want to understand how to measure data latency and lag in data replication. As a SRE I want to understand current thoughput for the replication solution.","title":"Performance  tests"},{"location":"compendium/","text":"Further Readings Event streams on Cloud documentation Kafka documentation Strimzi product documentation Prometheus Prometheus operator product documentation Grafana Ansible Performance test considerations","title":"Compendium"},{"location":"compendium/#further-readings","text":"Event streams on Cloud documentation Kafka documentation Strimzi product documentation Prometheus Prometheus operator product documentation Grafana Ansible Performance test considerations","title":"Further Readings"},{"location":"dc-local/","text":"Running a Kafka Cluster with Docker Compose We are providing a docker compose file to start a 3 broker cluster and 2 zookeeper nodes. In one Terminal window, start the local cluster using docker-compose under the local-cluster folder: docker-compose up & . The data are persisted on the local disk within this folder. If this is the first time you start this local Kafka cluster, you need to create the products topic. Start a Kafka container to access the Kafka tools with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify topic is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Your environment is up and running.","title":"Run local kafka cluster with docker compose"},{"location":"dc-local/#running-a-kafka-cluster-with-docker-compose","text":"We are providing a docker compose file to start a 3 broker cluster and 2 zookeeper nodes. In one Terminal window, start the local cluster using docker-compose under the local-cluster folder: docker-compose up & . The data are persisted on the local disk within this folder. If this is the first time you start this local Kafka cluster, you need to create the products topic. Start a Kafka container to access the Kafka tools with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify topic is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Your environment is up and running.","title":"Running a Kafka Cluster with Docker Compose"},{"location":"es-prem-to-es/","text":"From Event Streams on OCP to Event Streams on Cloud The source cluster is a IBM Event Streams cluster running on Openshift on-premise servers. It was installed following the instructions documented here . The target cluster is also IBM Event Streams on Cloud.","title":"From Event Streams on OpenShift to Event Streams on Cloud"},{"location":"es-prem-to-es/#from-event-streams-on-ocp-to-event-streams-on-cloud","text":"The source cluster is a IBM Event Streams cluster running on Openshift on-premise servers. It was installed following the instructions documented here . The target cluster is also IBM Event Streams on Cloud.","title":"From Event Streams on OCP to Event Streams on Cloud"},{"location":"es-to-local/","text":"To Event Streams to Kafka cluster on-premise Scenario 3: From Event Streams to local kafka cluster For this scenario the source is Event Streams on IBM Cloud and the target is a local Kafka cluster. As a prerequisite you need to run your local cluster, for example using docker compose as introduced in this note . This time the producer adds headers to the Records sent so we can validate headers replication. The file es-cluster/es-mirror-maker.properties declares the mirroring settings as below: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate. The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster. Notes If you have provisioned a mirror maker from the documentation here, you do not need to do the next step, but go to produce and consumer messages. Deploy mirror maker 2.0 with good configuration: As we use the properties file approach the Dockerfile helps us to build a custom MM2 with Prometheus JMX exporter and mm2.properties for configuration. The file specifies source and target cluster: clusters = source, target target.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 target.ssl.endpoint.identification.algorithm = source.bootstrap.servers = <REPLACE-WITH-ES-BROKERSLIST> source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.ssl.endpoint.identification.algorithm = https source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<REPLACE-WITH-ES-APIKEY>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Start Mirror Maker 2.0: we use the properties setting one with custom docker image. oc apply -f mirror-maker/mm2-deployment.yaml Produce some records to products topic on Event Streams. For that create a properties file ( eventstream.properties ) with the event streams API KEY and SASL_SSL properties: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"....\" Then starts a local container with Kafka 2.4 and console producer: export KAFKA_BROKERS = \"event streams broker list\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/eventstreams.properties --topic products\" > For the data, you can use any text, or the products we have in the data folder. To validate the target source.products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic source.products --from-beginning If you don ' t see a command prompt, try pressing enter. { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } Considerations When the source or target cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by the consumer or producer Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. To get the certificate do the following steps: Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS CA root certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt Transform the certificate for java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt","title":"Replication from Event Streams to local"},{"location":"es-to-local/#to-event-streams-to-kafka-cluster-on-premise","text":"","title":"To Event Streams to Kafka cluster on-premise"},{"location":"es-to-local/#scenario-3-from-event-streams-to-local-kafka-cluster","text":"For this scenario the source is Event Streams on IBM Cloud and the target is a local Kafka cluster. As a prerequisite you need to run your local cluster, for example using docker compose as introduced in this note . This time the producer adds headers to the Records sent so we can validate headers replication. The file es-cluster/es-mirror-maker.properties declares the mirroring settings as below: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate. The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders","title":"Scenario 3: From Event Streams to local kafka cluster"},{"location":"es-to-local/#scenario-4-from-event-streams-on-cloud-to-strimzi-cluster-on-openshift","text":"We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster. Notes If you have provisioned a mirror maker from the documentation here, you do not need to do the next step, but go to produce and consumer messages. Deploy mirror maker 2.0 with good configuration: As we use the properties file approach the Dockerfile helps us to build a custom MM2 with Prometheus JMX exporter and mm2.properties for configuration. The file specifies source and target cluster: clusters = source, target target.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 target.ssl.endpoint.identification.algorithm = source.bootstrap.servers = <REPLACE-WITH-ES-BROKERSLIST> source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.ssl.endpoint.identification.algorithm = https source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<REPLACE-WITH-ES-APIKEY>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Start Mirror Maker 2.0: we use the properties setting one with custom docker image. oc apply -f mirror-maker/mm2-deployment.yaml Produce some records to products topic on Event Streams. For that create a properties file ( eventstream.properties ) with the event streams API KEY and SASL_SSL properties: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"....\" Then starts a local container with Kafka 2.4 and console producer: export KAFKA_BROKERS = \"event streams broker list\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/eventstreams.properties --topic products\" > For the data, you can use any text, or the products we have in the data folder. To validate the target source.products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic source.products --from-beginning If you don ' t see a command prompt, try pressing enter. { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 }","title":"Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift"},{"location":"es-to-local/#considerations","text":"When the source or target cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by the consumer or producer Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. To get the certificate do the following steps: Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS CA root certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt Transform the certificate for java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt","title":"Considerations"},{"location":"local-to-es/","text":"From Local cluster to Event Streams Scenario 1: From Kafka local as source to Event Streams on Cloud as Target The test scenario goal is to send the product definitions in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY: export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. When defining APIKEy in Event streams you can have an admin, write or read access. So for Mirror Maker to create topics automatically it needs admin role. Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command to build this image (if you change the image name be sure to use the new name in future command) is: docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In this isolated python container bash shell, do the following command to send the first five products: $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams cluster properties file for the different Kafka tool commands. Set the password attribute of the jaas.config to match Event Streams APIKEY. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to Event Streams and with the replicated topic name: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0, close to the local cluster, which is your laptop, using yet another docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This launchMM2.sh script is updating a template properties file with the values of the environment variables and calls with this updated file: /opt/kafka/bin/connect-mirror-maker.sh mm2.properties The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder within the docker container. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving to the source.topics after the replication complete. { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Scenario 2: Run Mirror Maker 2 Cluster close to target cluster This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster: We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster (see this note to provision such environment). Producers are running locally on the same OpenShift cluster, where vanilla Kafka 2.4 is running, or can run remotely using exposed Kafka brokers Openshift route. The black rectangles in the figure above represent those producers. The goal is to replicate the products topic from the left to the source.products to the right. What needs to be done: Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction . It is used to deploy Mirror Maker 2, but for our test we use it as source cluster for replication too. Run Mirror Maker 2. See the detail in this provisioning note . Start consumer on source.products topic Run a producer to source topic named products Run Consumer To validate the replication works, we will connect a consumer to the source.products topic on Event Streams. So we define a target cluster property file ( eventstreams.properties ) like: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; Start the consumer on source.products topic running in Event Streams on the cloud: we use a setenv.sh shell to export the needed environment variables docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash bash-4.2$ source /home/setenv.sh bash-4.2$ ./bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Produce records to local cluster Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: export KAFKA_BROKERS = \"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.crt\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" As an alternate solution you can run the producer as a pod inside of the source cluster then send the product one by one using the console: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products If you don t see a command prompt, try pressing enter. > { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } > { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Note There is other solution to send records, like using a Kafka HTTP brigde and use curl post commands. To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning","title":"Replication from local to Event Streams"},{"location":"local-to-es/#from-local-cluster-to-event-streams","text":"","title":"From Local cluster to Event Streams"},{"location":"local-to-es/#scenario-1-from-kafka-local-as-source-to-event-streams-on-cloud-as-target","text":"The test scenario goal is to send the product definitions in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY: export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. When defining APIKEy in Event streams you can have an admin, write or read access. So for Mirror Maker to create topics automatically it needs admin role. Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command to build this image (if you change the image name be sure to use the new name in future command) is: docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In this isolated python container bash shell, do the following command to send the first five products: $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams cluster properties file for the different Kafka tool commands. Set the password attribute of the jaas.config to match Event Streams APIKEY. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to Event Streams and with the replicated topic name: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0, close to the local cluster, which is your laptop, using yet another docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This launchMM2.sh script is updating a template properties file with the values of the environment variables and calls with this updated file: /opt/kafka/bin/connect-mirror-maker.sh mm2.properties The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder within the docker container. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving to the source.topics after the replication complete. { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 }","title":"Scenario 1: From Kafka local as source to Event Streams on Cloud as Target"},{"location":"local-to-es/#scenario-2-run-mirror-maker-2-cluster-close-to-target-cluster","text":"This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster: We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster (see this note to provision such environment). Producers are running locally on the same OpenShift cluster, where vanilla Kafka 2.4 is running, or can run remotely using exposed Kafka brokers Openshift route. The black rectangles in the figure above represent those producers. The goal is to replicate the products topic from the left to the source.products to the right. What needs to be done: Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction . It is used to deploy Mirror Maker 2, but for our test we use it as source cluster for replication too. Run Mirror Maker 2. See the detail in this provisioning note . Start consumer on source.products topic Run a producer to source topic named products","title":"Scenario 2: Run Mirror Maker 2 Cluster close to target cluster"},{"location":"local-to-es/#run-consumer","text":"To validate the replication works, we will connect a consumer to the source.products topic on Event Streams. So we define a target cluster property file ( eventstreams.properties ) like: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; Start the consumer on source.products topic running in Event Streams on the cloud: we use a setenv.sh shell to export the needed environment variables docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash bash-4.2$ source /home/setenv.sh bash-4.2$ ./bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning","title":"Run Consumer"},{"location":"local-to-es/#produce-records-to-local-cluster","text":"Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: export KAFKA_BROKERS = \"my-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.crt\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" As an alternate solution you can run the producer as a pod inside of the source cluster then send the product one by one using the console: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic products If you don t see a command prompt, try pressing enter. > { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } > { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Note There is other solution to send records, like using a Kafka HTTP brigde and use curl post commands. To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning","title":"Produce records to local cluster"},{"location":"mm2-provisioning/","text":"Mirror Maker 2 Deployment In this article we are presenting different type of Mirror Maker 2 deployments. Updated 3/24 on Strimzi 017 rc4. Using Strimzi operator to deploy on Kubernetes To run in VM or docker image which can be adapted with your own configuration, like for example by adding prometheus JMX Exporter as java agent. We are using the configuration to deploy from event streams on Cloud to a local Kafka cluster we deployed using Strimzi. Common configuration When we need to create Kubernetes secrets to manage APIKEY to access Event Streams, and TLS certificate to access local Kafka brokers, we need to do the following steps: Create a project in OpenShift to deploy Mirror Maker cluster, for example: oc new-project <projectname> . Create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-api-secret --from-literal=password=<replace-with-event-streams-apikey> As your vanilla Kafka source cluster may use TLS to communicate between clients and brokers, you need to use the k8s secret defined when deploying Kafka which includes the CAroot certificate. This secret is : my-cluster-clients-ca-cert . # build a local CA crt file from the secret: oc extract secret/my-cluster-clients-ca-cert --keys = ca.crt --to = - > ca.crt # Verify the certificate: openssl x509 -in ca.crt -text # transform it for java truststore.jks: keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt # create a secret from file the truststore so it can be mounted as needed oc create secret generic kafka-truststore --from-file = ./truststore.jks # Verify the created secret oc describe secret kafka-truststore Attention At this step, we have two options to deploy mirror maker, one using the Mirror Maker Operator and configure it via a yaml file, or use properties file and a special docker image that is deployed to Openshift. As of 3/20/2020 we have found an issue on Strimzi 0.17-rc2 Mirror Maker 2.0 operator, so we are proposing to use the properties approach as documented this section . Deploying using Strimzi Mirror Maker operator We assume you have an existing namespace or project to deploy Mirror Maker. You also need to get the latest (0.17-rc4 at least) Strimzi configuration from the download page . If you have already installed Strimzi Operators, Cluster Roles, and CRDs, you do not need to do it again as those resources are defined at the kubernetes cluster level. See the provisioning note. Define source and target cluster properties in mirror maker 2.0 es-to-kafka-mm2.yml descriptor file. Here is the file for the replication between Event Streams and local cluster es-to-kafka-mm2.yml . We strongly recommend to study the schema definition of this custom resource from this page . Note connectCluster attribute defines the cluster alias used for Kafka Connect, it must match a cluster in the list at spec.clusters . The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3... tls : {} authentication : passwordSecret : secretName : es-api-secret password : password username : token type : plain Deploy Mirror maker 2.0 within your project. oc apply -f es-to-kafka-mm2.yml This commmand creates a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topics to replicate have multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers. Now with this deployment we can test consumer and producer as described in the scenario 4 . Deploying a custom Mirror Maker docker image We want to use custom docker image when we want to add Prometheus JMX exporter as Java Agent so we can monitor MM2 with Prometheus. The proposed docker file is in this folder and may look like: FROM strimzi/kafka:latest-kafka-2.4.0 # ... ENV LOG_DIR = /tmp/logs ENV EXTRA_ARGS = \"-javaagent:/usr/local/share/jars/jmx_prometheus_javaagent-0.12.0.jar=9400:/etc/jmx_exporter/jmx_exporter.yaml \" # .... EXPOSE 9400 CMD /opt/kafka/bin/connect-mirror-maker.sh /home/mm2.properties As the mirror maker 2 is using properties file, we want to define source and target cluster and the security settings for both clusters. As the goal is to run within the same OpenShift cluster as Kafka, the broker list for the source matches the URL within the broker service: # get the service URL oc describe svc my-cluster-kafka-bootstrap # URL my-cluster-kafka-bootstrap:9092 The target cluster uses the bootstrap servers from the Event Streams Credentials, and the API KEY is defined with the manager role, so mirror maker can create topic dynamically. Properties template file can be seen here: kafka-to-es-mm2 clusters = source, target source.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 source.ssl.endpoint.identification.algorithm = target.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username = \"token\" password=\"<Manager API KEY from Event Streams>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Upload the properties as a secret oc create secret generic mm2-std-properties --from-file = es-cluster/mm2.properties The file could be copied inside the docker image or better mounted from a secret when deployed to kubernetes. Build and push the image to a docker registry. docker build -t ibmcase/mm2ocp:v0.0.2 . docker push ibmcase/mm2ocp:v0.0.2 Then using a deployment configuration like this one , we can deploy our custom mirror maker 2 with: oc apply -f mm2-deployment.yaml # to assess the cluster oc get kafkamirrormaker2 NAME DESIRED REPLICAS mm2-cluster 1 Define the monitoring rules As explained in the monitoring note , we need to define the Prometheus rules within a yaml file so that Mirror Maker 2 can report metrics: lowercaseOutputName : true lowercaseOutputLabelNames : true rules : - pattern : \"kafka.connect<type=connect-worker-metrics>([^:]+):\" name : \"kafka_connect_connect_worker_metrics_$1\" - pattern : \"kafka.connect<type=connect-metrics, client-id=([^:]+)><>([^:]+)\" name : \"kafka_connect_connect_metrics_$1_$2\" # Rules below match the Kafka Connect/MirrorMaker MBeans in the jconsole order # Worker task states - pattern : kafka.connect<type=connect-worker-metrics, connector=(\\w+)><>(connector-destroyed-task-count|connector-failed-task-count|connector-paused-task-count|connector-running-task-count|connector-total-task-count|connector-unassigned-task-count) name : connect_worker_metrics_$1_$2 # Task metrics - pattern : kafka.connect<type=connector-task-metrics, connector=(\\w+), task=(\\d+)><>(batch-size-avg|batch-size-max|offset-commit-avg-time-ms|offset-commit-failure-percentage|offset-commit-max-time-ms|offset-commit-success-percentage|running-ratio) name : connect_connector_task_metrics_$1_$3 labels : task : \"$2\" # Source task metrics - pattern : kafka.connect<type=source-task-metrics, connector=(\\w+), task=(\\d+)><>(source-record-active-count|source-record-poll-total|source-record-write-total) name : connect_source_task_metrics_$1_$3 labels : task : \"$2\" # Task errors - pattern : kafka.connect<type=task-error-metrics, connector=(\\w+), task=(\\d+)><>(total-record-errors|total-record-failures|total-records-skipped|total-retries) name : connect_task_error_metrics_$1_$3 labels : task : \"$2\" # CheckpointConnector metrics - pattern : kafka.connect.mirror<type=MirrorCheckpointConnector, source=(.+), target=(.+), group=(.+), topic=(.+), partition=(\\d+)><>(checkpoint-latency-ms) name : connect_mirror_mirrorcheckpointconnector_$6 labels : source : \"$1\" target : \"$2\" group : \"$3\" topic : \"$4\" partition : \"$5\" # SourceConnector metrics - pattern : kafka.connect.mirror<type=MirrorSourceConnector, target=(.+), topic=(.+), partition=(\\d+)><>(byte-rate|byte-count|record-age-ms|record-rate|record-count|replication-latency-ms) name : connect_mirror_mirrorsourceconnector_$4 labels : target : \"$1\" topic : \"$2\" partition : \"$3\" Then upload this yaml file in a secret (the following command, represents a trick to update an existing configmap) oc create secret generic mm2-jmx-exporter --from-file = ./mm2-jmx-exporter.yaml Deploying on VM On virtual machine, it is possible to deploy the Apache Kafka 2.4+ binary file and then use the command /opt/kafka/bin/connect-mirror-maker.sh with the good properties file as argument. Within a VM we can run multiple mirror maker instances. When needed we can add more VMs to scale horizontally. Each mirror makers workers are part of the same consumer groups, so it is possible to scale at the limit of the topic partition number. Capacity planning We need to address some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. The parameters max.tasks specifies the max parallel processing we can have per JVM. So for each JVM we need to assess the number of partitions will be replicated. Each task is part of the same consumer group, so the load balancing will be transparent and is managed by the brokers. The task processing is stateless, consume - produce wait for acknowledge, commit offet. In this case the CPU and network are key. For platform tuning activity we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start to scale horizontally by adding mirror maker 2 instance. If the network at the server level is the bottleneck, then adding more servers will help. Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughtput as with small message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation. The parameters to consider for sizing are the following: Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s descrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency Deploying Mirror Maker 2 on its own project In this section we address another approach to, deploy a Kafka Connect cluster with Mirror Maker 2.0 connectors but without any local Kafka Cluster. The approach may be used with Event Streams on Cloud as backend Kafka cluster and Mirror Maker 2 for replication. Using the Strimzi operator we need to define a Yaml file for the connector and white and black lists for the topics to replicate. Here is an example of such descriptor . If we need to run a custom Mirror Maker 2, we have documented in the section above on how to use Dockerfile and properties file and deployment descriptor to do the deployment on kubernetes or OpenShift cluster. Provisioning automation For IT operation automation we can use Ansible to define a playbook to provision the Mirror Maker 2 environment. The Strimzi Ansible playbook repository containts playbook examples for creating cluster roles and service accounts and deploy operators. Typical errors in Mirror Maker 2 traces Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. This error message is a light issue in kafka 2.4 and does not impact the replication. In Kafka 2.5 this message is for DEBUG logs. Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}: Those messages may come from multiple reasons. One is that the named topic is not created. In Event Streams is the target cluster the topics may need to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election. The advertised listener may not be set or found. Exception on not being able to create Log directory: do the following: export LOG_DIR=/tmp/logs ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114) Some usefule commands Connect to local cluster: oc exec -ti eda-demo-24-cluster-kafka-0 bash list the topics: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list Get the description of the topics from one cluster: for t in $(./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --describe --topic $t done Create MM2 topics manually Here are some examples of command to create topic to the target cluster If you want to delete the topic on your local cluster for t in $(/opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --delete --topic $t done To create the topics manually on the target cluster:exit /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --topic mm2-offset-syncs.kafka-on-premise-cluster-source.internal /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --replication-factor 3 --topic mirrormaker2-cluster-status /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 25 --replication-factor 3 --topic mirrormaker2-cluster-offsets /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic mirrormaker2-cluster-configs /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic heartbeats /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 1 --topic event-streams-wdc.checkpoints.internal","title":"Mirror Maker 2 Provisioning"},{"location":"mm2-provisioning/#mirror-maker-2-deployment","text":"In this article we are presenting different type of Mirror Maker 2 deployments. Updated 3/24 on Strimzi 017 rc4. Using Strimzi operator to deploy on Kubernetes To run in VM or docker image which can be adapted with your own configuration, like for example by adding prometheus JMX Exporter as java agent. We are using the configuration to deploy from event streams on Cloud to a local Kafka cluster we deployed using Strimzi.","title":"Mirror Maker 2 Deployment"},{"location":"mm2-provisioning/#common-configuration","text":"When we need to create Kubernetes secrets to manage APIKEY to access Event Streams, and TLS certificate to access local Kafka brokers, we need to do the following steps: Create a project in OpenShift to deploy Mirror Maker cluster, for example: oc new-project <projectname> . Create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-api-secret --from-literal=password=<replace-with-event-streams-apikey> As your vanilla Kafka source cluster may use TLS to communicate between clients and brokers, you need to use the k8s secret defined when deploying Kafka which includes the CAroot certificate. This secret is : my-cluster-clients-ca-cert . # build a local CA crt file from the secret: oc extract secret/my-cluster-clients-ca-cert --keys = ca.crt --to = - > ca.crt # Verify the certificate: openssl x509 -in ca.crt -text # transform it for java truststore.jks: keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt # create a secret from file the truststore so it can be mounted as needed oc create secret generic kafka-truststore --from-file = ./truststore.jks # Verify the created secret oc describe secret kafka-truststore Attention At this step, we have two options to deploy mirror maker, one using the Mirror Maker Operator and configure it via a yaml file, or use properties file and a special docker image that is deployed to Openshift. As of 3/20/2020 we have found an issue on Strimzi 0.17-rc2 Mirror Maker 2.0 operator, so we are proposing to use the properties approach as documented this section .","title":"Common configuration"},{"location":"mm2-provisioning/#deploying-using-strimzi-mirror-maker-operator","text":"We assume you have an existing namespace or project to deploy Mirror Maker. You also need to get the latest (0.17-rc4 at least) Strimzi configuration from the download page . If you have already installed Strimzi Operators, Cluster Roles, and CRDs, you do not need to do it again as those resources are defined at the kubernetes cluster level. See the provisioning note. Define source and target cluster properties in mirror maker 2.0 es-to-kafka-mm2.yml descriptor file. Here is the file for the replication between Event Streams and local cluster es-to-kafka-mm2.yml . We strongly recommend to study the schema definition of this custom resource from this page . Note connectCluster attribute defines the cluster alias used for Kafka Connect, it must match a cluster in the list at spec.clusters . The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3... tls : {} authentication : passwordSecret : secretName : es-api-secret password : password username : token type : plain Deploy Mirror maker 2.0 within your project. oc apply -f es-to-kafka-mm2.yml This commmand creates a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topics to replicate have multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers. Now with this deployment we can test consumer and producer as described in the scenario 4 .","title":"Deploying using Strimzi Mirror Maker operator"},{"location":"mm2-provisioning/#deploying-a-custom-mirror-maker-docker-image","text":"We want to use custom docker image when we want to add Prometheus JMX exporter as Java Agent so we can monitor MM2 with Prometheus. The proposed docker file is in this folder and may look like: FROM strimzi/kafka:latest-kafka-2.4.0 # ... ENV LOG_DIR = /tmp/logs ENV EXTRA_ARGS = \"-javaagent:/usr/local/share/jars/jmx_prometheus_javaagent-0.12.0.jar=9400:/etc/jmx_exporter/jmx_exporter.yaml \" # .... EXPOSE 9400 CMD /opt/kafka/bin/connect-mirror-maker.sh /home/mm2.properties As the mirror maker 2 is using properties file, we want to define source and target cluster and the security settings for both clusters. As the goal is to run within the same OpenShift cluster as Kafka, the broker list for the source matches the URL within the broker service: # get the service URL oc describe svc my-cluster-kafka-bootstrap # URL my-cluster-kafka-bootstrap:9092 The target cluster uses the bootstrap servers from the Event Streams Credentials, and the API KEY is defined with the manager role, so mirror maker can create topic dynamically. Properties template file can be seen here: kafka-to-es-mm2 clusters = source, target source.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 source.ssl.endpoint.identification.algorithm = target.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username = \"token\" password=\"<Manager API KEY from Event Streams>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Upload the properties as a secret oc create secret generic mm2-std-properties --from-file = es-cluster/mm2.properties The file could be copied inside the docker image or better mounted from a secret when deployed to kubernetes. Build and push the image to a docker registry. docker build -t ibmcase/mm2ocp:v0.0.2 . docker push ibmcase/mm2ocp:v0.0.2 Then using a deployment configuration like this one , we can deploy our custom mirror maker 2 with: oc apply -f mm2-deployment.yaml # to assess the cluster oc get kafkamirrormaker2 NAME DESIRED REPLICAS mm2-cluster 1","title":"Deploying a custom Mirror Maker docker image"},{"location":"mm2-provisioning/#define-the-monitoring-rules","text":"As explained in the monitoring note , we need to define the Prometheus rules within a yaml file so that Mirror Maker 2 can report metrics: lowercaseOutputName : true lowercaseOutputLabelNames : true rules : - pattern : \"kafka.connect<type=connect-worker-metrics>([^:]+):\" name : \"kafka_connect_connect_worker_metrics_$1\" - pattern : \"kafka.connect<type=connect-metrics, client-id=([^:]+)><>([^:]+)\" name : \"kafka_connect_connect_metrics_$1_$2\" # Rules below match the Kafka Connect/MirrorMaker MBeans in the jconsole order # Worker task states - pattern : kafka.connect<type=connect-worker-metrics, connector=(\\w+)><>(connector-destroyed-task-count|connector-failed-task-count|connector-paused-task-count|connector-running-task-count|connector-total-task-count|connector-unassigned-task-count) name : connect_worker_metrics_$1_$2 # Task metrics - pattern : kafka.connect<type=connector-task-metrics, connector=(\\w+), task=(\\d+)><>(batch-size-avg|batch-size-max|offset-commit-avg-time-ms|offset-commit-failure-percentage|offset-commit-max-time-ms|offset-commit-success-percentage|running-ratio) name : connect_connector_task_metrics_$1_$3 labels : task : \"$2\" # Source task metrics - pattern : kafka.connect<type=source-task-metrics, connector=(\\w+), task=(\\d+)><>(source-record-active-count|source-record-poll-total|source-record-write-total) name : connect_source_task_metrics_$1_$3 labels : task : \"$2\" # Task errors - pattern : kafka.connect<type=task-error-metrics, connector=(\\w+), task=(\\d+)><>(total-record-errors|total-record-failures|total-records-skipped|total-retries) name : connect_task_error_metrics_$1_$3 labels : task : \"$2\" # CheckpointConnector metrics - pattern : kafka.connect.mirror<type=MirrorCheckpointConnector, source=(.+), target=(.+), group=(.+), topic=(.+), partition=(\\d+)><>(checkpoint-latency-ms) name : connect_mirror_mirrorcheckpointconnector_$6 labels : source : \"$1\" target : \"$2\" group : \"$3\" topic : \"$4\" partition : \"$5\" # SourceConnector metrics - pattern : kafka.connect.mirror<type=MirrorSourceConnector, target=(.+), topic=(.+), partition=(\\d+)><>(byte-rate|byte-count|record-age-ms|record-rate|record-count|replication-latency-ms) name : connect_mirror_mirrorsourceconnector_$4 labels : target : \"$1\" topic : \"$2\" partition : \"$3\" Then upload this yaml file in a secret (the following command, represents a trick to update an existing configmap) oc create secret generic mm2-jmx-exporter --from-file = ./mm2-jmx-exporter.yaml","title":"Define the monitoring rules"},{"location":"mm2-provisioning/#deploying-on-vm","text":"On virtual machine, it is possible to deploy the Apache Kafka 2.4+ binary file and then use the command /opt/kafka/bin/connect-mirror-maker.sh with the good properties file as argument. Within a VM we can run multiple mirror maker instances. When needed we can add more VMs to scale horizontally. Each mirror makers workers are part of the same consumer groups, so it is possible to scale at the limit of the topic partition number.","title":"Deploying on VM"},{"location":"mm2-provisioning/#capacity-planning","text":"We need to address some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. The parameters max.tasks specifies the max parallel processing we can have per JVM. So for each JVM we need to assess the number of partitions will be replicated. Each task is part of the same consumer group, so the load balancing will be transparent and is managed by the brokers. The task processing is stateless, consume - produce wait for acknowledge, commit offet. In this case the CPU and network are key. For platform tuning activity we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start to scale horizontally by adding mirror maker 2 instance. If the network at the server level is the bottleneck, then adding more servers will help. Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughtput as with small message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation. The parameters to consider for sizing are the following: Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s descrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency","title":"Capacity planning"},{"location":"mm2-provisioning/#deploying-mirror-maker-2-on-its-own-project","text":"In this section we address another approach to, deploy a Kafka Connect cluster with Mirror Maker 2.0 connectors but without any local Kafka Cluster. The approach may be used with Event Streams on Cloud as backend Kafka cluster and Mirror Maker 2 for replication. Using the Strimzi operator we need to define a Yaml file for the connector and white and black lists for the topics to replicate. Here is an example of such descriptor . If we need to run a custom Mirror Maker 2, we have documented in the section above on how to use Dockerfile and properties file and deployment descriptor to do the deployment on kubernetes or OpenShift cluster.","title":"Deploying Mirror Maker 2 on its own project"},{"location":"mm2-provisioning/#provisioning-automation","text":"For IT operation automation we can use Ansible to define a playbook to provision the Mirror Maker 2 environment. The Strimzi Ansible playbook repository containts playbook examples for creating cluster roles and service accounts and deploy operators.","title":"Provisioning automation"},{"location":"mm2-provisioning/#typical-errors-in-mirror-maker-2-traces","text":"Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. This error message is a light issue in kafka 2.4 and does not impact the replication. In Kafka 2.5 this message is for DEBUG logs. Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}: Those messages may come from multiple reasons. One is that the named topic is not created. In Event Streams is the target cluster the topics may need to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election. The advertised listener may not be set or found. Exception on not being able to create Log directory: do the following: export LOG_DIR=/tmp/logs ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114) Some usefule commands Connect to local cluster: oc exec -ti eda-demo-24-cluster-kafka-0 bash list the topics: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list Get the description of the topics from one cluster: for t in $(./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --describe --topic $t done","title":"Typical errors in Mirror Maker 2 traces"},{"location":"mm2-provisioning/#create-mm2-topics-manually","text":"Here are some examples of command to create topic to the target cluster If you want to delete the topic on your local cluster for t in $(/opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --delete --topic $t done To create the topics manually on the target cluster:exit /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --topic mm2-offset-syncs.kafka-on-premise-cluster-source.internal /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --replication-factor 3 --topic mirrormaker2-cluster-status /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 25 --replication-factor 3 --topic mirrormaker2-cluster-offsets /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic mirrormaker2-cluster-configs /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic heartbeats /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 1 --topic event-streams-wdc.checkpoints.internal","title":"Create MM2 topics manually"},{"location":"monitoring/","text":"Monitoring Mirror Maker and kafka connect cluster The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics using Prometheus and Grafana for the dashboard. Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. The following figure presents the prometheus generic architecture as described from the product main website. Basically the Prometheus server hosts job to poll HTTP end points to get metrics from the components to monitor. It supports queries in the format of PromQL , that product like Grafana can use to present nice dashboards, and it can push alerts to different channels when some metrics behave unexpectedly. In the context of data replication between kafka clusters, we want to monitor the mirror maker 2.0 metrics like the worker task states, source task metrics, task errors,... The following figure illustrates the components involved: The source Kafka cluster, the Mirror Maker 2.0 cluster, which is based on Kafka Connect, the Prometheus server and the Grafana. As all those components run on kubernetes, most of them could be deployed via Operators using Custom Resource Definitions. To support this monitoring we need to do the following steps: Add metrics configuration to your Mirror Maker 2.0 cluster Package the mirror maker 2 to use JMX Exporter as Java agent so it exposes JMX MBeans as metrics accessibles via HTTP. Deploy Prometheus via Opertors Optionally deploy Prometheus Alertmanager Deploy Grafana Installation and configuration Prometheus deployment inside Kubernetes uses operator as defined in the coreos github . The CRDs define a set of resources: the ServiceMonitor, PodMonitor, and PrometheusRule. Inside the Strimzi github repository , we can get a prometheus.yml file to deploy prometheus server using the Prometheus operator . This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance. We have defined our own configuration in this file . For your own deployment you have to change the target namespace, and the rules You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the Mirror Maker 2 Cluster. To be able to monitor your own on-premise Kafka cluster, you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment with Prometheus setting can be found in our kafka cluster definition . The declarations are under the metrics stanza and define the rules for exposing Kafka core features. Install Prometheus Operator We recommend reading Prometheus operator product documentation. At a glance the Prometheus operator deploy and manage a prometheus server and watches new pods to monitor when they are scheduled within k8s. Source: prometheus-operator architecture After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator and related service account, cluster role, role binding... We have reuse the monitoring/install/bundle.yaml from Prometheus operator github, with a namespace sets for our project (e.g jb-kafka-strimzi ): oc apply -f bundle.yaml When you apply those configurations, the following resources are visibles: Resource Description ClusterRole To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To manage the configuration of the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource) To delete the operator do: oc delete -f bundle.yaml Deploy prometheus Note The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker 2 or Kafka Connect monitoring, the configuration will have less rules, and parameters. See next section . Deploy the prometheus server by first changing the namespace and also by adapting the original examples/metrics/prometheus-install/prometheus.yaml file . curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: jb-kafka-strimzi/\" > prometheus.yml If you are using AlertManager (see section below ) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-rules.yaml oc apply -f prometheus-rules.yaml oc apply -f prometheus.yaml # once deploye, get the state of the server with oc get prometheus NAME VERSION REPLICAS AGE prometheus 1 52s The Prometheus server configuration uses service discovery to discover the pods (Mirror Maker 2.0 pod or kafka, zookeeper pods) in the cluster from which it gets metrics. Configure monitoring To start monitoring our Kafka 2.4 cluster we need to add some monitoring prometheus scrapper definitions, named service monitoring. oc apply -f strimzi-service-monitor.yaml oc describe servicemonitor Mirror maker 2.0 monitoring To monitor MM2 with Prometheus we need to add JMX Exporter and run it as Java agent.The jar file for JMX exporter agent can be found here . We copied a version in the folder mirror-maker-2/libs . We have adopted a custom mirror maker 2.0 docker imaged based on Kafka 2.4. We are detailing how to build this image using this Dockerfile in this separate note . We have used this approach as we have found an issue with the Strimzi Mirror Maker operator, that blocks us to continue the monitoring. We expect that htis operator, when it sees metrics declaration in the Mirror Maker 2 configuration yaml file, with use the JMX exporter jar. Once the Mirror Maker 2.0 is connected... Install Grafana Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker. Deploy Grafan to OpenShift and expose it via a service: oc apply -f grafana.yaml In case you want to test grafana locally run: docker run -d -p 3000:3000 grafana/grafana Kafka Explorer Configure Grafana dashboard To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service. Use port forwarding: export PODNAME = $( oc get pods -l name = grafana | grep grafana | awk '{print $1}' ) kubectl port-forward $PODNAME 3000 :3000 Point your browser to http://localhost:3000 . Expose the route via cli Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090 Alert Manager As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file The prometheus-rules.yaml . Those rules are used by the AlertManager component. Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager. Download an example of alert manager configuration file curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml > alert-manager.yaml Define a configuration for the channel to use, by starting from the following template curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml > alert-manager-config.yaml Modify this file to reflect the remote access credential and URL to the channel server. Then deploy the secret that matches your config file . oc create secret generic alertmanager-alertmanager --from-file = alertmanager.yaml = alert-manager-config.yaml oc create secret generic additional-scrape-configs --from-file = ./local-cluster/prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f - Further Readings Monitoring Event Streams cluster health with Prometheus","title":"Monitoring with Prometheus and Grafana"},{"location":"monitoring/#monitoring-mirror-maker-and-kafka-connect-cluster","text":"The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics using Prometheus and Grafana for the dashboard. Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. The following figure presents the prometheus generic architecture as described from the product main website. Basically the Prometheus server hosts job to poll HTTP end points to get metrics from the components to monitor. It supports queries in the format of PromQL , that product like Grafana can use to present nice dashboards, and it can push alerts to different channels when some metrics behave unexpectedly. In the context of data replication between kafka clusters, we want to monitor the mirror maker 2.0 metrics like the worker task states, source task metrics, task errors,... The following figure illustrates the components involved: The source Kafka cluster, the Mirror Maker 2.0 cluster, which is based on Kafka Connect, the Prometheus server and the Grafana. As all those components run on kubernetes, most of them could be deployed via Operators using Custom Resource Definitions. To support this monitoring we need to do the following steps: Add metrics configuration to your Mirror Maker 2.0 cluster Package the mirror maker 2 to use JMX Exporter as Java agent so it exposes JMX MBeans as metrics accessibles via HTTP. Deploy Prometheus via Opertors Optionally deploy Prometheus Alertmanager Deploy Grafana","title":"Monitoring Mirror Maker and kafka connect cluster"},{"location":"monitoring/#installation-and-configuration","text":"Prometheus deployment inside Kubernetes uses operator as defined in the coreos github . The CRDs define a set of resources: the ServiceMonitor, PodMonitor, and PrometheusRule. Inside the Strimzi github repository , we can get a prometheus.yml file to deploy prometheus server using the Prometheus operator . This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance. We have defined our own configuration in this file . For your own deployment you have to change the target namespace, and the rules You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the Mirror Maker 2 Cluster. To be able to monitor your own on-premise Kafka cluster, you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment with Prometheus setting can be found in our kafka cluster definition . The declarations are under the metrics stanza and define the rules for exposing Kafka core features.","title":"Installation and configuration"},{"location":"monitoring/#install-prometheus-operator","text":"We recommend reading Prometheus operator product documentation. At a glance the Prometheus operator deploy and manage a prometheus server and watches new pods to monitor when they are scheduled within k8s.","title":"Install Prometheus Operator"},{"location":"monitoring/#source-prometheus-operator-architecture","text":"After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator and related service account, cluster role, role binding... We have reuse the monitoring/install/bundle.yaml from Prometheus operator github, with a namespace sets for our project (e.g jb-kafka-strimzi ): oc apply -f bundle.yaml When you apply those configurations, the following resources are visibles: Resource Description ClusterRole To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To manage the configuration of the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource) To delete the operator do: oc delete -f bundle.yaml","title":"Source: prometheus-operator architecture"},{"location":"monitoring/#deploy-prometheus","text":"Note The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker 2 or Kafka Connect monitoring, the configuration will have less rules, and parameters. See next section . Deploy the prometheus server by first changing the namespace and also by adapting the original examples/metrics/prometheus-install/prometheus.yaml file . curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: jb-kafka-strimzi/\" > prometheus.yml If you are using AlertManager (see section below ) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-rules.yaml oc apply -f prometheus-rules.yaml oc apply -f prometheus.yaml # once deploye, get the state of the server with oc get prometheus NAME VERSION REPLICAS AGE prometheus 1 52s The Prometheus server configuration uses service discovery to discover the pods (Mirror Maker 2.0 pod or kafka, zookeeper pods) in the cluster from which it gets metrics.","title":"Deploy prometheus"},{"location":"monitoring/#configure-monitoring","text":"To start monitoring our Kafka 2.4 cluster we need to add some monitoring prometheus scrapper definitions, named service monitoring. oc apply -f strimzi-service-monitor.yaml oc describe servicemonitor","title":"Configure monitoring"},{"location":"monitoring/#mirror-maker-20-monitoring","text":"To monitor MM2 with Prometheus we need to add JMX Exporter and run it as Java agent.The jar file for JMX exporter agent can be found here . We copied a version in the folder mirror-maker-2/libs . We have adopted a custom mirror maker 2.0 docker imaged based on Kafka 2.4. We are detailing how to build this image using this Dockerfile in this separate note . We have used this approach as we have found an issue with the Strimzi Mirror Maker operator, that blocks us to continue the monitoring. We expect that htis operator, when it sees metrics declaration in the Mirror Maker 2 configuration yaml file, with use the JMX exporter jar. Once the Mirror Maker 2.0 is connected...","title":"Mirror maker 2.0 monitoring"},{"location":"monitoring/#install-grafana","text":"Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker. Deploy Grafan to OpenShift and expose it via a service: oc apply -f grafana.yaml In case you want to test grafana locally run: docker run -d -p 3000:3000 grafana/grafana","title":"Install Grafana"},{"location":"monitoring/#kafka-explorer","text":"","title":"Kafka Explorer"},{"location":"monitoring/#configure-grafana-dashboard","text":"To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service. Use port forwarding: export PODNAME = $( oc get pods -l name = grafana | grep grafana | awk '{print $1}' ) kubectl port-forward $PODNAME 3000 :3000 Point your browser to http://localhost:3000 . Expose the route via cli Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090","title":"Configure Grafana dashboard"},{"location":"monitoring/#alert-manager","text":"As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file The prometheus-rules.yaml . Those rules are used by the AlertManager component. Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager. Download an example of alert manager configuration file curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml > alert-manager.yaml Define a configuration for the channel to use, by starting from the following template curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml > alert-manager-config.yaml Modify this file to reflect the remote access credential and URL to the channel server. Then deploy the secret that matches your config file . oc create secret generic alertmanager-alertmanager --from-file = alertmanager.yaml = alert-manager-config.yaml oc create secret generic additional-scrape-configs --from-file = ./local-cluster/prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f -","title":"Alert Manager"},{"location":"monitoring/#further-readings","text":"Monitoring Event Streams cluster health with Prometheus","title":"Further Readings"},{"location":"perf-tests/","text":"Performance tests Tools used Test approach Measurements","title":"Performance Tests"},{"location":"perf-tests/#performance-tests","text":"","title":"Performance tests"},{"location":"perf-tests/#tools-used","text":"","title":"Tools used"},{"location":"perf-tests/#test-approach","text":"","title":"Test approach"},{"location":"perf-tests/#measurements","text":"","title":"Measurements"},{"location":"provisioning/","text":"Strimzi Operator and Kafka Cluster Provisioning In this note we propose to describe the provisioning of a Kafka Cluster using Strimzi operators and how to provision Mirror Maker 2 on Kubernetes or on VM. Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. The base of strimzi is to define a set of kubernetes operators and custom resource definitions for the different elements of Kafka. We recommend to go over the product overview page . The service account and role binding do not need to be re-installed if you did it previously. Concept summary The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics Example of topic can be seen in section below . Kafka User are not saved part of kafka cluster but they are managed in kubernetes. For example the user credentials are saved as secret. CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster, and are provided with Strimzi for each Kafka component used in a deployment. Strimzi Operators Deployment The deployment is done in two phases: Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resource to deploy. Deploy one to many instance of those CRDs In CR yaml file the kind attribute specifies the CRD to conform to. Each CRD has a common configuration like bootstrap servers, CPU resources, logging, healthchecks... The next steps are defining how to deploy a Kafka Cluster. Create a namespace or openshift project kubectl create namespace eda-strimzi-kafka24 # Or using Openshift CLI oc new-project eda-strimzi-kafka24 Download the strimzi artefacts We have already created the configuration from the source strimzi github in the following folder openshift-strimzi/eda-strimzi-kafka24 . So you do not need to do the following steps if you use the project: eda-strimzi-kafka24 . In case you want to do on your own, get the last Strimzi release from this github page . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: eda-strimzi-kafka24 /' $strimzi -home/install/cluster-operator/*RoleBinding*.yaml Deploy the Custom Resource Definitions for kafka Custom resource definitions are defined within the kubernetes cluster. The following commands oc apply -f openshift-strimzi/eda-strimzi-kafka24/cluster-operator/ oc get crd In case of Strimzi cluster operator fails with error like: \" kafkas.kafka.strimzi.io is forbidden: User \"system:serviceaccount:eda-strimzi-kafka24 :strimzi-cluster-operator\" cannot watch resource \"kafkas\" in API group \"kafka.strimzi.io\" in the namespace \"eda-strimzi-kafka24 \", you need to add cluster role to the strimzi operator user by doing the following commands: oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 oc adm policy add-cluster-role-to-user strimzi-entity-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 The commands above, should create the following service account, resource definitions, roles, and role bindings: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definitions oc get customresourcedefinition Add Strimzi Admin Role If you want to allow non-kubernetes cluster administators to manage Strimzi resources, you must assign them the Strimzi Administrator role. First deploy the role definition using the folowing command: oc apply -f openshift-strimzi/eda-strimzi-kafka24/010-ClusterRole-strimzi-admin.yaml Then assign the strimzi-admin ClusterRole to one or more existing users in the Kubernetes cluster. kubectl create clusterrolebinding strimzi-admin --clusterrole=strimzi-admin --user=<user-your-username-here> Deploy instances Deploy Kafka cluster The CRD for kafka cluster resource is here and we recommend to study it before defining your own cluster. Change the name of the cluster in one the yaml in the examples/kafka folder or use the openshift-strimzi/kafka-cluster.yml file in this project. This file defines the default replication factor of 3 and in-synch replicas of 2. For development purpose we will accept plain (unencrypted) listener on port 9092 without TLS authentication. For external to the kubernetes cluster access we need to have external listeners. For Openshift, as we use routes, we need to add the external.type = route . When exposing Kafka using OpenShift Routes and the HAProxy router, a dedicated Route is created for every Kafka broker pod. An additional Route is created to serve as a Kafka bootstrap address. Kafka clients can use these Routes to connect to Kafka on port 443. Even for development we added the metrics rules to expose kafka and zookeeper metrics for tool like Prometheus. For production we need to use persistence for the kafka log, ingress or load balancer external listener and rack awareness policies. It has to use Mutual TLS authentication, and with Strimzi we can use the User Operator to manage cluster users. Mutual authentication or two-way authentication is when both the server and the client present certificates. Using non presistence: oc apply -f openshift-strimzi/kafka-cluster.yaml oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 When looking at the pods running we can see the three kafka and zookeeper nodes as pods, and the entity operator pod. $ oc get pods my-cluster-entity-operator-645fdbc4cb-m29nk 3 /3 Running 0 18d my-cluster-kafka-0 2 /2 Running 0 3d my-cluster-kafka-1 2 /2 Running 0 3d my-cluster-kafka-2 2 /2 Running 0 3d my-cluster-zookeeper-0 2 /2 Running 0 3d my-cluster-zookeeper-1 2 /2 Running 0 3d my-cluster-zookeeper-2 2 /2 Running 0 3d strimzi-cluster-operator-58cbbcb7d-bcqhm 1 /1 Running 2 18d strimzi-topic-operator-564654cb86-nbt58 1 /1 Running 1 18d To use persistence add persistence volume and declare the PVC in the yaml file and then reapply: oc apply -f strimzi/kafka-cluster.yaml Add Topic CRDs and operator This step is optional. Topic operator helps to manage Kafka topics via yaml configuration and get map the topics as kubernetes resources so a command like oc get kafkatopics returns the list of topics. The operator keep the resources and the kafka topic in synch. This allows you to declare a KafkaTopic as part of your application\u2019s deployment. To manage Kafka topics with operators, first modify the file 05-Deployment-strimzi-topic-operator.yaml to reflect your cluster name env : - name : STRIMZI_RESOURCE_LABELS value : \"strimzi.io/cluster=eda-demo-24-cluster\" - name : STRIMZI_KAFKA_BOOTSTRAP_SERVERS value : eda-demo-24-cluster-kafka-bootstrap:9092 - name : STRIMZI_ZOOKEEPER_CONNECT value : eda-demo-24-cluster-zookeeper-client:2181 and then deploy the topic-operator. This operation will fail if there is no Kafka Boker and Zookeeper available: oc apply -f openshift-strimzi/install/topic-operator oc adm policy add-cluster-role-to-user strimzi-topic-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition Create a topic Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : eda-demo-24-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml oc get kafkatopics This creates a topic test in your kafka cluster. Add User CRDs and operator This step is optional. To manage Kafka user with operators modify the file 05-Deployment-strimzi-user-operator.yaml to reflect your cluster name and then deploy the user-operator: oc apply -f openshift-strimzi/install/user-operator Test with producer and consumer pods Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts on your local computer, remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka custer definition yaml file includes the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt The alias is used to access keystore entries (key and trusted certificate entries). Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol = SSL --producer-property ssl.truststore.password = password --producer-property ssl.truststore.location = /home/truststore.jks --topic test Those properties can be in file bootstrap.servers = my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Strimzi Kafka 2.4 Provisioning on Openshift"},{"location":"provisioning/#strimzi-operator-and-kafka-cluster-provisioning","text":"In this note we propose to describe the provisioning of a Kafka Cluster using Strimzi operators and how to provision Mirror Maker 2 on Kubernetes or on VM. Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. The base of strimzi is to define a set of kubernetes operators and custom resource definitions for the different elements of Kafka. We recommend to go over the product overview page . The service account and role binding do not need to be re-installed if you did it previously.","title":"Strimzi Operator and Kafka Cluster Provisioning"},{"location":"provisioning/#concept-summary","text":"The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics Example of topic can be seen in section below . Kafka User are not saved part of kafka cluster but they are managed in kubernetes. For example the user credentials are saved as secret. CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster, and are provided with Strimzi for each Kafka component used in a deployment.","title":"Concept summary"},{"location":"provisioning/#strimzi-operators-deployment","text":"The deployment is done in two phases: Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resource to deploy. Deploy one to many instance of those CRDs In CR yaml file the kind attribute specifies the CRD to conform to. Each CRD has a common configuration like bootstrap servers, CPU resources, logging, healthchecks... The next steps are defining how to deploy a Kafka Cluster.","title":"Strimzi Operators Deployment"},{"location":"provisioning/#create-a-namespace-or-openshift-project","text":"kubectl create namespace eda-strimzi-kafka24 # Or using Openshift CLI oc new-project eda-strimzi-kafka24","title":"Create a namespace or openshift project"},{"location":"provisioning/#download-the-strimzi-artefacts","text":"We have already created the configuration from the source strimzi github in the following folder openshift-strimzi/eda-strimzi-kafka24 . So you do not need to do the following steps if you use the project: eda-strimzi-kafka24 . In case you want to do on your own, get the last Strimzi release from this github page . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: eda-strimzi-kafka24 /' $strimzi -home/install/cluster-operator/*RoleBinding*.yaml","title":"Download the strimzi artefacts"},{"location":"provisioning/#deploy-the-custom-resource-definitions-for-kafka","text":"Custom resource definitions are defined within the kubernetes cluster. The following commands oc apply -f openshift-strimzi/eda-strimzi-kafka24/cluster-operator/ oc get crd In case of Strimzi cluster operator fails with error like: \" kafkas.kafka.strimzi.io is forbidden: User \"system:serviceaccount:eda-strimzi-kafka24 :strimzi-cluster-operator\" cannot watch resource \"kafkas\" in API group \"kafka.strimzi.io\" in the namespace \"eda-strimzi-kafka24 \", you need to add cluster role to the strimzi operator user by doing the following commands: oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 oc adm policy add-cluster-role-to-user strimzi-entity-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 The commands above, should create the following service account, resource definitions, roles, and role bindings: Names Resource Command strimzi-cluster-operator Service account oc get sa strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Role oc get clusterrole strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definitions oc get customresourcedefinition","title":"Deploy the Custom Resource Definitions for kafka"},{"location":"provisioning/#add-strimzi-admin-role","text":"If you want to allow non-kubernetes cluster administators to manage Strimzi resources, you must assign them the Strimzi Administrator role. First deploy the role definition using the folowing command: oc apply -f openshift-strimzi/eda-strimzi-kafka24/010-ClusterRole-strimzi-admin.yaml Then assign the strimzi-admin ClusterRole to one or more existing users in the Kubernetes cluster. kubectl create clusterrolebinding strimzi-admin --clusterrole=strimzi-admin --user=<user-your-username-here>","title":"Add Strimzi Admin Role"},{"location":"provisioning/#deploy-instances","text":"","title":"Deploy instances"},{"location":"provisioning/#deploy-kafka-cluster","text":"The CRD for kafka cluster resource is here and we recommend to study it before defining your own cluster. Change the name of the cluster in one the yaml in the examples/kafka folder or use the openshift-strimzi/kafka-cluster.yml file in this project. This file defines the default replication factor of 3 and in-synch replicas of 2. For development purpose we will accept plain (unencrypted) listener on port 9092 without TLS authentication. For external to the kubernetes cluster access we need to have external listeners. For Openshift, as we use routes, we need to add the external.type = route . When exposing Kafka using OpenShift Routes and the HAProxy router, a dedicated Route is created for every Kafka broker pod. An additional Route is created to serve as a Kafka bootstrap address. Kafka clients can use these Routes to connect to Kafka on port 443. Even for development we added the metrics rules to expose kafka and zookeeper metrics for tool like Prometheus. For production we need to use persistence for the kafka log, ingress or load balancer external listener and rack awareness policies. It has to use Mutual TLS authentication, and with Strimzi we can use the User Operator to manage cluster users. Mutual authentication or two-way authentication is when both the server and the client present certificates. Using non presistence: oc apply -f openshift-strimzi/kafka-cluster.yaml oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 When looking at the pods running we can see the three kafka and zookeeper nodes as pods, and the entity operator pod. $ oc get pods my-cluster-entity-operator-645fdbc4cb-m29nk 3 /3 Running 0 18d my-cluster-kafka-0 2 /2 Running 0 3d my-cluster-kafka-1 2 /2 Running 0 3d my-cluster-kafka-2 2 /2 Running 0 3d my-cluster-zookeeper-0 2 /2 Running 0 3d my-cluster-zookeeper-1 2 /2 Running 0 3d my-cluster-zookeeper-2 2 /2 Running 0 3d strimzi-cluster-operator-58cbbcb7d-bcqhm 1 /1 Running 2 18d strimzi-topic-operator-564654cb86-nbt58 1 /1 Running 1 18d To use persistence add persistence volume and declare the PVC in the yaml file and then reapply: oc apply -f strimzi/kafka-cluster.yaml","title":"Deploy Kafka cluster"},{"location":"provisioning/#add-topic-crds-and-operator","text":"This step is optional. Topic operator helps to manage Kafka topics via yaml configuration and get map the topics as kubernetes resources so a command like oc get kafkatopics returns the list of topics. The operator keep the resources and the kafka topic in synch. This allows you to declare a KafkaTopic as part of your application\u2019s deployment. To manage Kafka topics with operators, first modify the file 05-Deployment-strimzi-topic-operator.yaml to reflect your cluster name env : - name : STRIMZI_RESOURCE_LABELS value : \"strimzi.io/cluster=eda-demo-24-cluster\" - name : STRIMZI_KAFKA_BOOTSTRAP_SERVERS value : eda-demo-24-cluster-kafka-bootstrap:9092 - name : STRIMZI_ZOOKEEPER_CONNECT value : eda-demo-24-cluster-zookeeper-client:2181 and then deploy the topic-operator. This operation will fail if there is no Kafka Boker and Zookeeper available: oc apply -f openshift-strimzi/install/topic-operator oc adm policy add-cluster-role-to-user strimzi-topic-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 This will add the following: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition","title":"Add Topic CRDs and operator"},{"location":"provisioning/#create-a-topic","text":"Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : test labels : strimzi.io/cluster : eda-demo-24-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 oc apply -f test.yaml oc get kafkatopics This creates a topic test in your kafka cluster.","title":"Create a topic"},{"location":"provisioning/#add-user-crds-and-operator","text":"This step is optional. To manage Kafka user with operators modify the file 05-Deployment-strimzi-user-operator.yaml to reflect your cluster name and then deploy the user-operator: oc apply -f openshift-strimzi/install/user-operator","title":"Add User CRDs and operator"},{"location":"provisioning/#test-with-producer-and-consumer-pods","text":"Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts on your local computer, remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka custer definition yaml file includes the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/my-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt The alias is used to access keystore entries (key and trusted certificate entries). Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer-property security.protocol = SSL --producer-property ssl.truststore.password = password --producer-property ssl.truststore.location = /home/truststore.jks --topic test Those properties can be in file bootstrap.servers = my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Test with producer and consumer pods"},{"location":"security/","text":"Security considerations org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [products]","title":"Security"},{"location":"security/#security-considerations","text":"org.apache.kafka.common.errors.TopicAuthorizationException: Not authorized to access topics: [products]","title":"Security considerations"}]}