{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This repository includes a set of documents for best practices around data replication between two Kafka clusters. Mirror Maker 2.0 Mirror Maker 2.0 is the new replication feature of Kafka 2.4. It was defined as part of the Kafka Improvement Process - KIP 382 . General concepts As Mirror maker 2.0 is using Kafka Connect framework, we recommend to review our summary of Kafka Connect in this note . The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect. In distributed mode, MirrorMaker 2.0 creates the following topics on the target cluster: mm2-configs.source.internal: This topic is used to store the connector and task configuration. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic is used to store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol. IBM Event Streams is a support, enterprise version of Apache Kafka by IBM. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 Topics are configured to be replicated or not using a whitelist and blacklist concept White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] We can also define the blacklist with the properties: topics.blacklist . Comma-separated lists and Java Regular Expressions are supported. Internally, MirrorSourceConnector and MirrorCheckpointConnector will create multiple Kafka tasks (up to the value of tasks.max property), and MirrorHeartbeatConnector creates an additional task. MirrorSourceConnector will have one task per topic-partition combination to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the assign() API, so there is no consumer group used while fetching data from source topics. There is no call to commit() either; rebalancing occurs only when there is a new topic created that matches the whitelist pattern. Requirements to address Environments We propose two approaches to run the on-premise Kafka cluster: Docker compose using vanilla Kafka 2.4 - This appraoch to running the local cluster uses Docker with Docker Compose. The Docker Compose file to start a local cluster with 3 Kafka Brokers and 2 Zookeepers is in mirror-maker-2/local-cluster folder. This Docker Compose file uses a local Docker network called kafkanet . The Docker image used for Kafka comes from the Strimzi open source project and is for Kafka version 2.4. We describe how to setup this simple cluster using Docker Compose in this article . Kafka 2.4 cluster using the Strimzi Operator deployed on Openshift - This approach to runnig the local cluster leverages the Strimzi Kubernetes Operator running on the OpenShift Container Platform. For the Event Streams on Cloud cluster, we recommend to create your own using an IBM Cloud account. The product documentation is here . The enviroments are summarized in the table below: Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks The connect column defines where the MirrorMaker 2 connect to. Local Kafka cluster to Event Streams on Cloud The goal is to demonstrate the replicate data from local Kafka cluster to Event Streams on IBM Cloud, which is running as managed service. The two scenarios and the step-by-step approach are presented in this note . We have documented the replication from Event Streams on IBM Cloud as a Service to a local Kafka cluster in this note with two scenarios depending on where the target Kafka cluster is running, either on OpenShift or on Docker. Provisioning Connectors (MirrorMaker 2) Thinking of our goals as Agile user stories, we list our stories and some notes and requirements below. As an SRE, I want to provision and deploy MirrorMaker 2 connector to existing Openshift cluster without exposing passwords and keys so replication can start working. This will use Kubernetes secrets for configuration parameters. We describe the MM2 deployment with secrets in this section . As an SRE I want to understand the CLI commands used to assess how the provisioning process can be automated. We did not show how to automate the deployment, but as all deployments are done with CLI and configuration files given, we could consider using Ansible for automation. As an SRE, I want to understand the server sizing for the Mirror Maker environment so that I can understand the leanest resources for minimal needs. We talk about capacity planning in this section and performance tests in a separate note . As an SRE I want to understand if Mirroring can run from older Kafka cluster version (1.1 +) to newer kafka cluster version. We do not have an old Kafka 1.1 cluster available to us for testing, but after discussion with development and doing deeper analysis, the Kafka Connect framework was already working with Kafka 1.1 and use the bootstrap server to connect to the cluster. So using an existing topic defined in the Kafka 1.1 cluster as source for mirroring will work. Note that, there is no specific user interface for MirrorMaker 2. Version-to-Version Migration As a SRE, I want to understand how to perform a version-to-version migration for the MirrorMaker 2 product so that existing streaming replication is not impacted by the upgrade. See some rolling upgrade recommendations in this section . As a developer I want to deploy configuration updates to modify the topic replication white or black lists so that newly added topics are replicated. Mirror maker 2 configuration is done with yaml files and applied using kubectl or oc CLI. The Strimzi cluster operator is managing the redeployment of the mirror maker deployment. Security As an SRE, I want to understand how client applications authenticate to source and target Kafka clusters. As a developer, I want to design MirrorMaker 2 based replication solution to support different lines of businesses who should not connect to topics and data not related to their business and security scope. Those subjects are addressed in the security note . Monitoring As an SRE, I want to get MirrorMaker 2 metrics for Prometheus so that it fits in my current metrics processing practices. The explanation of how to set up Prometheus metrics for MirrorMaker 2.0 is documented in the monitoring note . As an SRE, I want to be able to add new dashboards into Grafana to visualize the MirrorMaker 2 metrics. As an SRE, I want to define rules for alert reporting and configure a Slack channel for alerting. [Removed] As an SRE, I want to get the MirrorMaker 2 logs into our Splunk logging platform. Best Practices As a developer I want to understand how MirrorMaker 2 based replication addresses the record duplication. Here is a note on records duplication . As a developer I want to design the MirrorMaker 2 Kafka topic replication solution to use minimal resources but also be able to scale-up if I observe data replication lag. Some lag will always be present due to the the fact that MirrorMaker 2 does asynchronous replication, but it is possible to scale MirrorMaker 2 vertically and horizontally to minimize the lag As a developer I want to understand what are the conditions under which messages may be lost. We are detailing the exactly once and ensuring delivery in those notes: producer and consumer . Performance tests As a developer, I want to understand how to measure latency / lag in data replication. IBM Event streams offers an event producers to do stress testing. The performance tests are documented in this chapter . As a SRE I want to understand current throughput for the replication solution. Kafka brokers are reporting the number of request per second, and the byte in per sec as part of the metrics visible in Prometheus. Those metrics can be at the producer Fetch consumer or fetch follower level.","title":"Introduction"},{"location":"#introduction","text":"This repository includes a set of documents for best practices around data replication between two Kafka clusters.","title":"Introduction"},{"location":"#mirror-maker-20","text":"Mirror Maker 2.0 is the new replication feature of Kafka 2.4. It was defined as part of the Kafka Improvement Process - KIP 382 .","title":"Mirror Maker 2.0"},{"location":"#general-concepts","text":"As Mirror maker 2.0 is using Kafka Connect framework, we recommend to review our summary of Kafka Connect in this note . The figure below illustrates the MirrorMaker 2.0 internal components running within Kafka Connect. In distributed mode, MirrorMaker 2.0 creates the following topics on the target cluster: mm2-configs.source.internal: This topic is used to store the connector and task configuration. mm2-offsets.source.internal: This topic is used to store offsets for Kafka Connect. mm2-status.source.internal: This topic is used to store status updates of connectors and tasks. source.heartbeats source.checkpoints.internal A typical MirrorMaker 2.0 configuration is done via a property file and defines the replication source and target clusters with their connection properties and the replication flow definition. Here is a simple example for a local cluster replicating to a remote IBM Event Streams cluster using TLS v1.2 for connection encryption and SASL authentication protocol. IBM Event Streams is a support, enterprise version of Apache Kafka by IBM. clusters = source, target source.bootstrap.servers = ${KAFKA_SOURCE_BROKERS} target.bootstrap.servers = ${KAFKA_TARGET_BROKERS} target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=${KAFKA_TARGET_APIKEY}; # enable and configure individual replication flows source->target.enabled = true source->target.topics = products tasks.max = 10 Topics are configured to be replicated or not using a whitelist and blacklist concept White listed topics are set with the source->target.topics attribute of the replication flow and uses Java regular expression syntax. Blacklisted topics: by default the following pattern is applied: blacklist = [follower\\.replication\\.throttled\\.replicas, leader\\.replication\\.throttled\\.replicas, message\\.timestamp\\.difference\\.max\\.ms, message\\.timestamp\\.type, unclean\\.leader\\.election\\.enable, min\\.insync\\.replicas] We can also define the blacklist with the properties: topics.blacklist . Comma-separated lists and Java Regular Expressions are supported. Internally, MirrorSourceConnector and MirrorCheckpointConnector will create multiple Kafka tasks (up to the value of tasks.max property), and MirrorHeartbeatConnector creates an additional task. MirrorSourceConnector will have one task per topic-partition combination to replicate, while MirrorCheckpointConnector will have one task per consumer group. The Kafka Connect framework uses the coordinator API, with the assign() API, so there is no consumer group used while fetching data from source topics. There is no call to commit() either; rebalancing occurs only when there is a new topic created that matches the whitelist pattern.","title":"General concepts"},{"location":"#requirements-to-address","text":"","title":"Requirements to address"},{"location":"#environments","text":"We propose two approaches to run the on-premise Kafka cluster: Docker compose using vanilla Kafka 2.4 - This appraoch to running the local cluster uses Docker with Docker Compose. The Docker Compose file to start a local cluster with 3 Kafka Brokers and 2 Zookeepers is in mirror-maker-2/local-cluster folder. This Docker Compose file uses a local Docker network called kafkanet . The Docker image used for Kafka comes from the Strimzi open source project and is for Kafka version 2.4. We describe how to setup this simple cluster using Docker Compose in this article . Kafka 2.4 cluster using the Strimzi Operator deployed on Openshift - This approach to runnig the local cluster leverages the Strimzi Kubernetes Operator running on the OpenShift Container Platform. For the Event Streams on Cloud cluster, we recommend to create your own using an IBM Cloud account. The product documentation is here . The enviroments are summarized in the table below: Environment Source Target Connect 1 Local Event Streams on Cloud Local 2 Strimzi on OCP Event Streams on Cloud OCP / Roks 3 Event Streams on Cloud Local Local 4 Event Streams on Cloud Strimzi on OCP OCP/ Roks 5 Event Streams on OCP Event Streams on Cloud OCP / Roks The connect column defines where the MirrorMaker 2 connect to.","title":"Environments"},{"location":"#local-kafka-cluster-to-event-streams-on-cloud","text":"The goal is to demonstrate the replicate data from local Kafka cluster to Event Streams on IBM Cloud, which is running as managed service. The two scenarios and the step-by-step approach are presented in this note . We have documented the replication from Event Streams on IBM Cloud as a Service to a local Kafka cluster in this note with two scenarios depending on where the target Kafka cluster is running, either on OpenShift or on Docker.","title":"Local Kafka cluster to Event Streams on Cloud"},{"location":"#provisioning-connectors-mirrormaker-2","text":"Thinking of our goals as Agile user stories, we list our stories and some notes and requirements below. As an SRE, I want to provision and deploy MirrorMaker 2 connector to existing Openshift cluster without exposing passwords and keys so replication can start working. This will use Kubernetes secrets for configuration parameters. We describe the MM2 deployment with secrets in this section . As an SRE I want to understand the CLI commands used to assess how the provisioning process can be automated. We did not show how to automate the deployment, but as all deployments are done with CLI and configuration files given, we could consider using Ansible for automation. As an SRE, I want to understand the server sizing for the Mirror Maker environment so that I can understand the leanest resources for minimal needs. We talk about capacity planning in this section and performance tests in a separate note . As an SRE I want to understand if Mirroring can run from older Kafka cluster version (1.1 +) to newer kafka cluster version. We do not have an old Kafka 1.1 cluster available to us for testing, but after discussion with development and doing deeper analysis, the Kafka Connect framework was already working with Kafka 1.1 and use the bootstrap server to connect to the cluster. So using an existing topic defined in the Kafka 1.1 cluster as source for mirroring will work. Note that, there is no specific user interface for MirrorMaker 2.","title":"Provisioning Connectors (MirrorMaker 2)"},{"location":"#version-to-version-migration","text":"As a SRE, I want to understand how to perform a version-to-version migration for the MirrorMaker 2 product so that existing streaming replication is not impacted by the upgrade. See some rolling upgrade recommendations in this section . As a developer I want to deploy configuration updates to modify the topic replication white or black lists so that newly added topics are replicated. Mirror maker 2 configuration is done with yaml files and applied using kubectl or oc CLI. The Strimzi cluster operator is managing the redeployment of the mirror maker deployment.","title":"Version-to-Version Migration"},{"location":"#security","text":"As an SRE, I want to understand how client applications authenticate to source and target Kafka clusters. As a developer, I want to design MirrorMaker 2 based replication solution to support different lines of businesses who should not connect to topics and data not related to their business and security scope. Those subjects are addressed in the security note .","title":"Security"},{"location":"#monitoring","text":"As an SRE, I want to get MirrorMaker 2 metrics for Prometheus so that it fits in my current metrics processing practices. The explanation of how to set up Prometheus metrics for MirrorMaker 2.0 is documented in the monitoring note . As an SRE, I want to be able to add new dashboards into Grafana to visualize the MirrorMaker 2 metrics. As an SRE, I want to define rules for alert reporting and configure a Slack channel for alerting. [Removed] As an SRE, I want to get the MirrorMaker 2 logs into our Splunk logging platform.","title":"Monitoring"},{"location":"#best-practices","text":"As a developer I want to understand how MirrorMaker 2 based replication addresses the record duplication. Here is a note on records duplication . As a developer I want to design the MirrorMaker 2 Kafka topic replication solution to use minimal resources but also be able to scale-up if I observe data replication lag. Some lag will always be present due to the the fact that MirrorMaker 2 does asynchronous replication, but it is possible to scale MirrorMaker 2 vertically and horizontally to minimize the lag As a developer I want to understand what are the conditions under which messages may be lost. We are detailing the exactly once and ensuring delivery in those notes: producer and consumer .","title":"Best Practices"},{"location":"#performance-tests","text":"As a developer, I want to understand how to measure latency / lag in data replication. IBM Event streams offers an event producers to do stress testing. The performance tests are documented in this chapter . As a SRE I want to understand current throughput for the replication solution. Kafka brokers are reporting the number of request per second, and the byte in per sec as part of the metrics visible in Prometheus. Those metrics can be at the producer Fetch consumer or fetch follower level.","title":"Performance  tests"},{"location":"compendium/","text":"Further Readings Event streams on Cloud documentation Kafka documentation Strimzi product documentation Strimzi MirrorMaker 2 introduction Prometheus Prometheus operator product documentation Grafana Ansible Performance test considerations Cloudera kafka replication a case for MM2 IBM practices - Monitoring of EDA Apps - general discussions Open telemetry a Cloud Native Computing Foundation project for monitoring app","title":"Compendium"},{"location":"compendium/#further-readings","text":"Event streams on Cloud documentation Kafka documentation Strimzi product documentation Strimzi MirrorMaker 2 introduction Prometheus Prometheus operator product documentation Grafana Ansible Performance test considerations Cloudera kafka replication a case for MM2 IBM practices - Monitoring of EDA Apps - general discussions Open telemetry a Cloud Native Computing Foundation project for monitoring app","title":"Further Readings"},{"location":"consideration/","text":"Replication considerations Why replicating The classical needs for replication between clusters can be bullet listed as: Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals. Active active cluster mirroring for inter services communication: consumers and producers are on both side and consumer or produce to their local cluster. Moving data to read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view. GDPR compliance to isolate data in country and geography Hybrid cloud operations to share data between on-premise cluster and managed service clusters. Topic metadata replication It is possible to disable the topic metadata replication (The configuration is ``). We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare. By synchronizing configuration properties, the need for rebalancing is reduced. When doing manual configuration, even if the initial topic configuration was duplicated, any dynamic changes to the topic properties are not going to be automatically propagate and the administrator needs to change the target topic. If the throughput on the source topic has increase and the number of partition was increased to support the load, then the target cluster will not have the same downstream capability which may lead to overloading (disk space or memory capavity). Also if the consumer of a partition is expecting to process the event in order within the partition, then changing the number of partition between source and target will make the ordering not valid any more. Also if the replication factor are set differently between the two clusters then the availability guarantees of the replicated data may be impacted and bad settings with broker failure will lead to data lost. Finally it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates a backlog in the pipeline and increases the end to end latency observed by the downstream application. Naming convention Mirror maker 2 prefix the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid infinite loop when doing bi-directional mirroring. At the consumer side the subscribe() function support regular expression for topic name. So a code like: kafkaConsumer . subscribe ( \"^.*accounts\" ) will listen to all the topics in the cluster having cluster name prefix and local accounts topics. This could be useful when we want to aggregate data from different data centers / clusters. Offset management Mirror maker 2 track offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its record it gets the offset in the partition the record was saved. In the diagram below we have a source topic/partition A with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as 12 in the target partition. offset # do not match between partitions. So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. This information is saved in the checkpoint topic. Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example the normal behavior is increase the offset by one 2,3,4,5,6,7 is mapped to 12,13,14,15,16,... if the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the offset-synch topic. The checkpoint and offset_synch topics enable replication to be fully restored from the correct offset position on failover. Record duplication Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka producer, brokers, and consumers are working together to ensure only one message is processed end to end. With coding practice and configuration, within a unique cluster, Kafka can guarantee exactly once processing. No duplication between producer and broker, and committed read on consumer side is not reprocessed in case of consumer restarts. Cross cluster replications are traditionally based on at least once approach. Duplicates can happen when consumer task stops before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. The following diagram illustrate this case: As mirror maker is a generic topic consumer, it will not participate to a read-committed process, if the topic includes duplicate messages it will propagate to the target. In the future MM2 will be able to support exactly once by using the checkpoint topic on the target cluster to keep the state of the committed offset from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part of the same transaction. For consumer coding We recommend to review the producer implementation best practices and the consumer considerations . For platform sizing, the main metric to assess, is the number of partitions in the cluster to replicate. The number of partitions and number of brokers are somehow connected as getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.","title":"Replicaton considerations"},{"location":"consideration/#replication-considerations","text":"","title":"Replication considerations"},{"location":"consideration/#why-replicating","text":"The classical needs for replication between clusters can be bullet listed as: Disaster recovery when one secondary cluster is passive while the producer and consumers are on the active cluster in the primary data center: The following article goes over those principals. Active active cluster mirroring for inter services communication: consumers and producers are on both side and consumer or produce to their local cluster. Moving data to read only cluster as a front door to data lake, or to do cross data centers aggregation on the different event streams: Fan-in to get holistic data view. GDPR compliance to isolate data in country and geography Hybrid cloud operations to share data between on-premise cluster and managed service clusters.","title":"Why replicating"},{"location":"consideration/#topic-metadata-replication","text":"It is possible to disable the topic metadata replication (The configuration is ``). We do not encourage to do so. Per design topic can be added dynamically, specially when developing with Kafka Streams where intermediate topics are created, and topic configuration can be altered to increase the number of partitions. Changes to the source topic are dynamically propagated to the target avoiding maintenance nightmare. By synchronizing configuration properties, the need for rebalancing is reduced. When doing manual configuration, even if the initial topic configuration was duplicated, any dynamic changes to the topic properties are not going to be automatically propagate and the administrator needs to change the target topic. If the throughput on the source topic has increase and the number of partition was increased to support the load, then the target cluster will not have the same downstream capability which may lead to overloading (disk space or memory capavity). Also if the consumer of a partition is expecting to process the event in order within the partition, then changing the number of partition between source and target will make the ordering not valid any more. Also if the replication factor are set differently between the two clusters then the availability guarantees of the replicated data may be impacted and bad settings with broker failure will lead to data lost. Finally it is important to consider that changes to topic configuration triggers a consumer rebalance which stalls the mirroring process and creates a backlog in the pipeline and increases the end to end latency observed by the downstream application.","title":"Topic metadata replication"},{"location":"consideration/#naming-convention","text":"Mirror maker 2 prefix the name of the replicated topic with the name of the source cluster. This is an important and simple solution to avoid infinite loop when doing bi-directional mirroring. At the consumer side the subscribe() function support regular expression for topic name. So a code like: kafkaConsumer . subscribe ( \"^.*accounts\" ) will listen to all the topics in the cluster having cluster name prefix and local accounts topics. This could be useful when we want to aggregate data from different data centers / clusters.","title":"Naming convention"},{"location":"consideration/#offset-management","text":"Mirror maker 2 track offset per consumer group. There are two topics created on the target cluster to manage the offset mapping between the source and target clusters and the checkpoints of the last committed offset in the source topic/partitions/consumer group. When a producer sends its record it gets the offset in the partition the record was saved. In the diagram below we have a source topic/partition A with the last write offset done by a producer to be 5, and the last read committed offset by the consumer assigned to partition 1 being 3. The last replicated offset 3 is mapped as 12 in the target partition. offset # do not match between partitions. So if the blue consumer needs to reconnect to the green target cluster it will read from the last committed offset which is 12 in this environment. This information is saved in the checkpoint topic. Offset synch are emitted at the beginning of the replication and when there is a situation which leads that the numbering sequencing diverges. For example the normal behavior is increase the offset by one 2,3,4,5,6,7 is mapped to 12,13,14,15,16,... if the write operation for offset 20 at the source is a 17 on the target then MM 2 emits a new offset synch records to the offset-synch topic. The checkpoint and offset_synch topics enable replication to be fully restored from the correct offset position on failover.","title":"Offset management"},{"location":"consideration/#record-duplication","text":"Exactly-once delivery is difficult to achieve in distributed system. In the case of Kafka producer, brokers, and consumers are working together to ensure only one message is processed end to end. With coding practice and configuration, within a unique cluster, Kafka can guarantee exactly once processing. No duplication between producer and broker, and committed read on consumer side is not reprocessed in case of consumer restarts. Cross cluster replications are traditionally based on at least once approach. Duplicates can happen when consumer task stops before committing its offset to the source topic. A restart will load records from the last committed offset which can generate duplicates. The following diagram illustrate this case: As mirror maker is a generic topic consumer, it will not participate to a read-committed process, if the topic includes duplicate messages it will propagate to the target. In the future MM2 will be able to support exactly once by using the checkpoint topic on the target cluster to keep the state of the committed offset from the consumer side, and write with an atomic transaction between the target topic and the checkpoint topic, and commit the source read offset as part of the same transaction.","title":"Record duplication"},{"location":"consideration/#for-consumer-coding","text":"We recommend to review the producer implementation best practices and the consumer considerations . For platform sizing, the main metric to assess, is the number of partitions in the cluster to replicate. The number of partitions and number of brokers are somehow connected as getting a high number of partitions involves increasing the number of brokers. For Mirror Maker 2, as it is based on Kafka connect, there is a unique cluster and each partition mirroring is supported by a task within the JVM so the first constraint is the memory allocated to the container and the heap size.","title":"For consumer coding"},{"location":"dc-local/","text":"Running a Kafka Cluster with Docker Compose We are providing a docker compose file to start a local 3 Broker Kafka cluster with 2 Zookeeper nodes. In one Terminal window, start the local cluster using docker-compose under the local-cluster folder: docker-compose up & . The data are persisted on the local disk within this folder. If this is the first time you start this local Kafka cluster, you need to create the products topic. Start a Kafka container to access the Kafka tools with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify topic is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Your environment is up and running.","title":"Run local kafka cluster with docker compose"},{"location":"dc-local/#running-a-kafka-cluster-with-docker-compose","text":"We are providing a docker compose file to start a local 3 Broker Kafka cluster with 2 Zookeeper nodes. In one Terminal window, start the local cluster using docker-compose under the local-cluster folder: docker-compose up & . The data are persisted on the local disk within this folder. If this is the first time you start this local Kafka cluster, you need to create the products topic. Start a Kafka container to access the Kafka tools with the command: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash Then in the bash shell, go to /home/local-cluster folder and execute the script: ./createProductsTopic.sh . Verify topic is created with the command: /opt/kafka/bin/kafka-topics.sh --bootstrap-server kafka1:9092 --list Your environment is up and running.","title":"Running a Kafka Cluster with Docker Compose"},{"location":"es-prem-to-es/","text":"From Event Streams on OCP to Event Streams on Cloud The source cluster is a IBM Event Streams cluster running on Openshift on-premise servers. It was installed following the instructions documented here . The target cluster is also IBM Event Streams on Cloud.","title":"From Event Streams on OpenShift to Event Streams on Cloud"},{"location":"es-prem-to-es/#from-event-streams-on-ocp-to-event-streams-on-cloud","text":"The source cluster is a IBM Event Streams cluster running on Openshift on-premise servers. It was installed following the instructions documented here . The target cluster is also IBM Event Streams on Cloud.","title":"From Event Streams on OCP to Event Streams on Cloud"},{"location":"es-to-local/","text":"To Event Streams to Kafka cluster on-premise Scenario 3: From Event Streams to local kafka cluster For this scenario the source is Event Streams on IBM Cloud and the target is a local Kafka cluster. As a prerequisite you need to run your local cluster, for example using docker compose as introduced in this note . This time the producer adds headers to the Records sent so we can validate headers replication. The file es-cluster/es-mirror-maker.properties declares the mirroring settings as below: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate. The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster. Notes If you have provisioned a mirror maker from the documentation here, you do not need to do the next step, but go to produce and consumer messages. Deploy mirror maker 2.0 with good configuration: As we use the properties file approach the Dockerfile helps us to build a custom MM2 with Prometheus JMX exporter and mm2.properties for configuration. The file specifies source and target cluster: clusters = source, target target.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 target.ssl.endpoint.identification.algorithm = source.bootstrap.servers = <REPLACE-WITH-ES-BROKERSLIST> source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.ssl.endpoint.identification.algorithm = https source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<REPLACE-WITH-ES-APIKEY>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Start Mirror Maker 2.0: we use the properties setting one with custom docker image. oc apply -f mirror-maker/mm2-deployment.yaml Produce some records to products topic on Event Streams. For that create a properties file ( eventstream.properties ) with the event streams API KEY and SASL_SSL properties: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"....\" Then starts a local container with Kafka 2.4 and console producer: export KAFKA_BROKERS = \"event streams broker list\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/eventstreams.properties --topic products\" > For the data, you can use any text, or the products we have in the data folder. To validate the target source.products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --topic source.products --from-beginning If you don ' t see a command prompt, try pressing enter. { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } Considerations When the source or target cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by the consumer or producer Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. See this section for the steps to build the truststore.","title":"Replication from Event Streams to local"},{"location":"es-to-local/#to-event-streams-to-kafka-cluster-on-premise","text":"","title":"To Event Streams to Kafka cluster on-premise"},{"location":"es-to-local/#scenario-3-from-event-streams-to-local-kafka-cluster","text":"For this scenario the source is Event Streams on IBM Cloud and the target is a local Kafka cluster. As a prerequisite you need to run your local cluster, for example using docker compose as introduced in this note . This time the producer adds headers to the Records sent so we can validate headers replication. The file es-cluster/es-mirror-maker.properties declares the mirroring settings as below: clusters = source, target source.bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"985...\"; target.bootstrap.servers = kafka1:9092,kafka2:9093,kafka3:9094 # enable and configure individual replication flows source->target.enabled = true source->target.topics = orders Start mirror maker2.0 : By using a new container, start another kakfa 2.4+ docker container, connected to the brokers via the kafkanet network, and mounting the configuration in the /home : docker run -ti --network kafkanet -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash Inside this container starts mirror maker 2.0 using the script: /opt/kakfa/bin/connect-mirror-maker.sh /opt/kakfa/bin/connect-mirror-maker.sh /home/strimzi-mm2.properties The strimzi-mm2.properties properties file given as argument defines the source and target clusters and the topics to replicate. The consumer may be started in second or third step. To start it, you can use a new container or use one of the running kafka broker container. Using the Docker perspective in Visual Code, we can get into a bash shell within one of the Kafka broker container. The local folder is mounted to /home . Then the script, consumeFromLocal.sh source.orders will get messages from the replicated topic: source.orders","title":"Scenario 3: From Event Streams to local kafka cluster"},{"location":"es-to-local/#scenario-4-from-event-streams-on-cloud-to-strimzi-cluster-on-openshift","text":"We are reusing the Event Streams on Cloud cluster on Washington DC data center as source target and the vanilla Kafka 2.4 cluster as target, also running within Washington data center in a OpenShift Cluster. As both clusters are in the same data center, we deploy Mirror Maker 2.0 close to target kafka cluster. Notes If you have provisioned a mirror maker from the documentation here, you do not need to do the next step, but go to produce and consumer messages. Deploy mirror maker 2.0 with good configuration: As we use the properties file approach the Dockerfile helps us to build a custom MM2 with Prometheus JMX exporter and mm2.properties for configuration. The file specifies source and target cluster: clusters = source, target target.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 target.ssl.endpoint.identification.algorithm = source.bootstrap.servers = <REPLACE-WITH-ES-BROKERSLIST> source.security.protocol = SASL_SSL source.ssl.protocol = TLSv1.2 source.ssl.endpoint.identification.algorithm = https source.sasl.mechanism = PLAIN source.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<REPLACE-WITH-ES-APIKEY>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Start Mirror Maker 2.0: we use the properties setting one with custom docker image. oc apply -f mirror-maker/mm2-deployment.yaml Produce some records to products topic on Event Streams. For that create a properties file ( eventstream.properties ) with the event streams API KEY and SASL_SSL properties: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"....\" Then starts a local container with Kafka 2.4 and console producer: export KAFKA_BROKERS = \"event streams broker list\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/eventstreams.properties --topic products\" > For the data, you can use any text, or the products we have in the data folder. To validate the target source.products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --topic source.products --from-beginning If you don ' t see a command prompt, try pressing enter. { \"product_id\" : \"P01\" , \"description\" : \"Carrots\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 } { \"product_id\" : \"P02\" , \"description\" : \"Banana\" , \"target_temperature\" : 6 , \"target_humidity_level\" : 0 .6, \"content_type\" : 2 } { \"product_id\" : \"P03\" , \"description\" : \"Salad\" , \"target_temperature\" : 4 , \"target_humidity_level\" : 0 .4, \"content_type\" : 1 }","title":"Scenario 4: From Event Streams On Cloud to Strimzi Cluster on Openshift"},{"location":"es-to-local/#considerations","text":"When the source or target cluster is deployed on Openshift, the exposed route to access the brokers is using TLS connection. So we need the certificate and create a truststore to be used by the consumer or producer Java programs. All kafka tools are done in java or scala so running in a JVM, which needs truststore for keep trusted TLS certificates. See this section for the steps to build the truststore.","title":"Considerations"},{"location":"local-to-es/","text":"From Local cluster to Event Streams Scenario 1: From Kafka local as source to Event Streams on Cloud as Target The test scenario goal is to send the product definitions in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY: export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. When defining APIKEy in Event streams you can have an admin, write or read access. So for Mirror Maker to create topics automatically it needs admin role. Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command to build this image (if you change the image name be sure to use the new name in future command) is: docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In this isolated python container bash shell, do the following command to send the first five products: $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams cluster properties file for the different Kafka tool commands. Set the password attribute of the jaas.config to match Event Streams APIKEY. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to Event Streams and with the replicated topic name: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0, close to the local cluster, which is your laptop, using yet another docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This launchMM2.sh script is updating a template properties file with the values of the environment variables and calls with this updated file: /opt/kafka/bin/connect-mirror-maker.sh mm2.properties The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder within the docker container. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving to the source.topics after the replication complete. { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Scenario 2: Run Mirror Maker 2 Cluster close to target cluster This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster: We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster (see this note to provision such environment). Producers are running locally on the same OpenShift cluster, where vanilla Kafka 2.4 is running, or can run remotely using exposed Kafka brokers Openshift route. The black rectangles in the figure above represent those producers. The goal is to replicate the products topic from the left to the source.products to the right. What needs to be done: Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction . It is used to deploy Mirror Maker 2, but for our test we use it as source cluster for replication too. Get the API KEY with manager role for event streams cluster and define a kubernetes secret: Run Mirror Maker 2 with the configuration as define in the file: ``. See next section for starting the MME or, read also some mirror maker 2 deployment details in this provisioning note . Start consumer on source.products topic Run a producer to source topic named products Run mirror maker 2 Create a topic to the target cluster: mm2-offset-syncs.kafka-on-premise-cluster.internal oc apply -f local-cluster/kafka-to-es-mm2.yml A new pod is created or if you have an existing mirror maker 2 depoyed the new configuration is added to your cluster. Run Consumer To validate the replication works, we will connect a consumer to the source.products topic on Event Streams. So we define a target cluster property file ( eventstreams.properties ) like: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; Start the consumer on source.products topic running in Event Streams on the cloud: we use a setenv.sh shell to export the needed environment variables docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash bash-4.2$ source /home/setenv.sh bash-4.2$ ./bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/mirror-maker-2/eventstream.properties --topic source.products --from-beginning Produce records to local cluster Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: # Under the mirror-maker-2 folder export KAFKA_BROKERS = \"eda-demo-24-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.crt\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" As an alternate solution you can run the producer as a pod inside of the source cluster then send the product one by one using the console: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap:9092 --topic products If you don t see a command prompt, try pressing enter. > { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } > { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Note There is other solution to send records, like using a Kafka HTTP brigde and use curl post commands. To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning","title":"Replication from local to Event Streams"},{"location":"local-to-es/#from-local-cluster-to-event-streams","text":"","title":"From Local cluster to Event Streams"},{"location":"local-to-es/#scenario-1-from-kafka-local-as-source-to-event-streams-on-cloud-as-target","text":"The test scenario goal is to send the product definitions in the local products topic and then start mirror maker to see the data replicated to the source.products topic in Event Streams cluster. Set the environment variables in setenv.sh script for the source broker to be your local cluster, and the target to be event streams. Be sure to also set Event Streams APIKEY: export KAFKA_SOURCE_BROKERS = kafka1:9092,kafka2:9093,kafka3:9094 export KAFKA_TARGET_BROKERS = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_TARGET_APIKEY = \"<password attribut in event streams credentials>\" It may be needed to create the topics in the target cluster. This depends if mirror maker 2.0 is able to access the AdminClient API. When defining APIKEy in Event streams you can have an admin, write or read access. So for Mirror Maker to create topics automatically it needs admin role. Send some products data to this topic. For that we use a docker python image. The docker file to build this image is python-kafka/Dockerfile-python so the command to build this image (if you change the image name be sure to use the new name in future command) is: docker build -f Dockerfile-python -t jbcodeforce/python37 . Once the image is built, start the python environment with the following commands: source ./setenv.sh docker run -ti -v $( pwd ) :/home --rm -e KAFKA_BROKERS = $KAFKA_SOURCE_BROKERS --network kafkanet jbcodeforce/python37 bash In this isolated python container bash shell, do the following command to send the first five products: $ echo $KAFKA_BROKERS kafka1:9092,kafka2:9093,kafka3:9094 $ python SendProductToKafka.py ./data/products.json [ KafkaProducer ] - { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] [ KafkaProducer ] - Message delivered to products [ 0 ] To validate the data are in the source topic we can use the kafka console consumer. Here are the basic commands: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ cd bin $ ./kafka-console-consumer.sh --bootstrap-server kafka1:9092 --topic products --from-beginning Define the event streams cluster properties file for the different Kafka tool commands. Set the password attribute of the jaas.config to match Event Streams APIKEY. The eventstream.properties file looks like: bootstrap.servers = broker-3-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-qnprtqnp7hnkssdz.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=....; Restart the kafka-console-consumer with the bootstrap URL to access to Event Streams and with the replicated topic name: source.products . Use the previously created properties file to get authentication properties so the command looks like: source /home/setenv.sh ./kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/eventstream.properties --topic source.products --from-beginning Now we are ready to start Mirror Maker 2.0, close to the local cluster, which is your laptop, using yet another docker image: docker run -ti -v $( pwd ) :/home --network kafkanet strimzi/kafka:latest-kafka-2.4.0 bash $ /home/local-cluster/launchMM2.sh This launchMM2.sh script is updating a template properties file with the values of the environment variables and calls with this updated file: /opt/kafka/bin/connect-mirror-maker.sh mm2.properties The trace includes a ton of messages, which displays different Kafka connect consumers and producers, workers and tasks. The logs can be found in the /tmp/logs folder within the docker container. The table includes some of the elements of this configuration: Name Description Worker clientId=connect-2, groupId=target-mm2 Herder for target cluster topics but reading source topic Producer clientId=producer-1 Producer to taget cluster Consumer clientId=consumer-target-mm2-1, groupId=target-mm2] Subscribed to 25 partition(s): mm2-offsets.target.internal-0 to 24 Consumer clientId=consumer-target-mm2-2, groupId=target-mm2] Subscribed to 5 partition(s): mm2-status.target.internal-0 to 4 Consumer clientId=consumer-target-mm2-3, groupId=target-mm2] Subscribed to partition(s): mm2-configs.target.internal-0 Worker clientId=connect-2, groupId=target-mm2 . Starting connectors and tasks using config offset 6. This trace shows mirror maker will start to consume message from the offset 6. A previous run has already committed the offset for this client id. This illustrate a Mirror Maker restarts Starting connector MirrorHeartbeatConnector and Starting task MirrorHeartbeatConnector-0 Starting connector MirrorCheckpointConnector Starting connector MirrorSourceConnector As expected, in the consumer console we can see the 5 product messages arriving to the source.topics after the replication complete. { 'bootstrap.servers' : 'kafka1:9092,kafka2:9093,kafka3:9094' , 'group.id' : 'ProductsProducer' } { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 }","title":"Scenario 1: From Kafka local as source to Event Streams on Cloud as Target"},{"location":"local-to-es/#scenario-2-run-mirror-maker-2-cluster-close-to-target-cluster","text":"This scenario is similar to the scenario 1 but Mirror Maker 2.0 now, runs within an OpenShift cluster in the same data center as Event Streams cluster, so closer to the target cluster: We have created an Event Streams cluster on Washington DC data center. We have Strimzi operators deployed in Washington data center OpenShift Cluster (see this note to provision such environment). Producers are running locally on the same OpenShift cluster, where vanilla Kafka 2.4 is running, or can run remotely using exposed Kafka brokers Openshift route. The black rectangles in the figure above represent those producers. The goal is to replicate the products topic from the left to the source.products to the right. What needs to be done: Get a OpenShift cluster in the same data center as Event Streams service: See this product introduction . It is used to deploy Mirror Maker 2, but for our test we use it as source cluster for replication too. Get the API KEY with manager role for event streams cluster and define a kubernetes secret: Run Mirror Maker 2 with the configuration as define in the file: ``. See next section for starting the MME or, read also some mirror maker 2 deployment details in this provisioning note . Start consumer on source.products topic Run a producer to source topic named products","title":"Scenario 2: Run Mirror Maker 2 Cluster close to target cluster"},{"location":"local-to-es/#run-mirror-maker-2","text":"Create a topic to the target cluster: mm2-offset-syncs.kafka-on-premise-cluster.internal oc apply -f local-cluster/kafka-to-es-mm2.yml A new pod is created or if you have an existing mirror maker 2 depoyed the new configuration is added to your cluster.","title":"Run mirror maker 2"},{"location":"local-to-es/#run-consumer","text":"To validate the replication works, we will connect a consumer to the source.products topic on Event Streams. So we define a target cluster property file ( eventstreams.properties ) like: security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"am_...\"; Start the consumer on source.products topic running in Event Streams on the cloud: we use a setenv.sh shell to export the needed environment variables docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash bash-4.2$ source /home/setenv.sh bash-4.2$ ./bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TARGET_BROKERS --consumer.config /home/mirror-maker-2/eventstream.properties --topic source.products --from-beginning","title":"Run Consumer"},{"location":"local-to-es/#produce-records-to-local-cluster","text":"Start a producer to send product records to the source Kafka cluster. If you have done the scenario 1, the first product definitions may be already in the target cluster, so we can send a second batch of products using a second data file: # Under the mirror-maker-2 folder export KAFKA_BROKERS = \"eda-demo-24-cluster-kafka-bootstrap-jb-kafka-strimzi.gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443\" export KAFKA_CERT = \"/home/ca.crt\" docker run -ti -v $( pwd ) :/home --rm -e KAFKA_CERT = $KAFKA_CERT -e KAFKA_BROKERS = $KAFKA_BROKERS strimzi/kafka:latest-kafka-2.4.0 bash -c \"/opt/kafka/bin/kafka-console-producer.sh --broker-list $KAFKA_BROKERS --producer.config /home/kafka-strimzi.properties --topic products\" As an alternate solution you can run the producer as a pod inside of the source cluster then send the product one by one using the console: oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap:9092 --topic products If you don t see a command prompt, try pressing enter. > { 'product_id' : 'P01' , 'description' : 'Carrots' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P02' , 'description' : 'Banana' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .6, 'content_type' : 2 } > { 'product_id' : 'P03' , 'description' : 'Salad' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P04' , 'description' : 'Avocado' , 'target_temperature' : 6 , 'target_humidity_level' : 0 .4, 'content_type' : 1 } > { 'product_id' : 'P05' , 'description' : 'Tomato' , 'target_temperature' : 4 , 'target_humidity_level' : 0 .4, 'content_type' : 2 } Note There is other solution to send records, like using a Kafka HTTP brigde and use curl post commands. To validate the source products topic has records, start a consumer as pod on Openshift within the source Kafka cluster using the Strimzi/kafka image. oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic products --from-beginning","title":"Produce records to local cluster"},{"location":"mm2-provisioning/","text":"Mirror Maker 2 Deployment In this article we are presenting different type of Mirror Maker 2 deployments. Updated 4/4 on Strimzi version 0.17. Using Strimzi operator to deploy on Kubernetes To run MM2 on a VM or as docker image which can be adapted with your own configuration, like for example by adding prometheus JMX Exporter as java agent. We are using the configuration to deploy from event streams on Cloud to a local Kafka cluster we deployed using Strimzi. Common configuration When we need to create Kubernetes Secrets to manage APIKEY to access Event Streams, and TLS certificate to access local Kafka brokers, we need to do the following steps: Create a project in OpenShift to deploy Mirror Maker cluster, for example: oc new-project <projectname> . Create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-api-secret --from-literal=password=<replace-with-event-streams-apikey> As your vanilla Kafka source cluster may use TLS to communicate between clients and brokers, you need to use the k8s secret defined when deploying Kafka which includes the CAroot and generic client certificates. These secrets are : eda-demo-24-cluster-clients-ca-cert and eda-demo-24-cluster-cluster-ca-cert . Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS CA root certificate from the broker oc get secrets oc extract secret/eda-demo-24-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt oc extract secret/eda-demo-24-cluster-clients-ca-cert --keys = ca.crt --to = - >> ca.crt Transform the certificates for java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt Create a secret from truststore file so it can be mounted as needed into consumer or producer running in the same OpenShift cluster. oc create secret generic kafka-truststore --from-file = ./truststore.jks Deploying using Strimzi Mirror Maker operator We assume you have an existing namespace or project to deploy Mirror Maker. You also need to get the latest (0.17-rc4 at least) Strimzi configuration from the download page . If you have already installed Strimzi Operators, Cluster Roles, and CRDs, you do not need to do it again as those resources are defined at the kubernetes cluster level. See the provisioning note. Define source and target cluster properties in mirror maker 2.0 es-to-kafka-mm2.yml descriptor file. Here is the file for the replication between Event Streams and local cluster es-to-kafka-mm2.yml . We strongly recommend to study the schema definition of this custom resource from this page . Note connectCluster attribute defines the cluster alias used for Kafka Connect, it must match a cluster in the list at spec.clusters . The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3... tls : {} authentication : passwordSecret : secretName : es-api-secret password : password username : token type : plain The example above use a secret to get the Event Streams API KEY, which as create with a command like: oc create secret generic es-api-secret --from-literal=password=<replace with ES key> Deploy Mirror maker 2.0 within your project. oc apply -f es-to-kafka-mm2.yml This commmand creates a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topics to replicate have multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers. Now with this deployment we can test consumer and producer as described in the scenario 4 . MM2 topology In this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi tenancy. The simplest approach is to use one MirrorMaker instance per familly of topics: the classification of familly of topic can be anything, from line of business, to team, to application. Suppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application responsible to manage the topics replication. The configuration will define the groupId to match the application name for example. The following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three MirrorMaker 2 instances mirroring the different topics with name starting with topic-name-A*, topic-name-B*, topic-name-C*, respectively. Each connect instance is a JVM workers that replicate the topic/parititions and has different group.id. For Bi-directional replication for the same topic name, MirrorMaker 2 will use the cluster name as prefix. The following example is showing the configuration for one MM2 connector: apiVersion : kafka . strimzi . io / v1alpha1 kind : KafkaMirrorMaker2 Capacity planning To address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. The parameters max.tasks specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated. Each task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controler. With Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller. So the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing. The task processing is stateless, consume - produce wait for acknowledge, commit offet. In this case the CPU and network are key. For platform tuning activity we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start to scale horizontally by adding mirror maker 2 instance. If the network at the server level is the bottleneck, then adding more servers will help. Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughtput as with small message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation. The parameters to consider for sizing are the following: Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s descrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency Version migration Once the MirrorMaker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves via Kafka topic. If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers. If a new worker starts work, a rebalance ensures it takes over some work from the existing workers. Using the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the 'consumer' task to partition assignment. When using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough. Be sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version. Deploying a custom MirrorMaker docker image We want to use custom docker image when we want to add Prometheus JMX exporter as Java Agent so we can monitor MM2 with Prometheus. The proposed docker file is in this folder and may look like: FROM strimzi/kafka:latest-kafka-2.4.0 # ... ENV LOG_DIR = /tmp/logs ENV EXTRA_ARGS = \"-javaagent:/usr/local/share/jars/jmx_prometheus_javaagent-0.12.0.jar=9400:/etc/jmx_exporter/jmx_exporter.yaml \" # .... EXPOSE 9400 CMD /opt/kafka/bin/connect-mirror-maker.sh /home/mm2.properties As the mirror maker 2 is using properties file, we want to define source and target cluster and the security settings for both clusters. As the goal is to run within the same OpenShift cluster as Kafka, the broker list for the source matches the URL within the broker service: # get the service URL oc describe svc my-cluster-kafka-bootstrap # URL my-cluster-kafka-bootstrap:9092 The target cluster uses the bootstrap servers from the Event Streams Credentials, and the API KEY is defined with the manager role, so mirror maker can create topic dynamically. Properties template file can be seen here: kafka-to-es-mm2 clusters = source, target source.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 source.ssl.endpoint.identification.algorithm = target.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username = \"token\" password=\"<Manager API KEY from Event Streams>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Upload the properties as a secret oc create secret generic mm2-std-properties --from-file = es-cluster/mm2.properties The file could be copied inside the docker image or better mounted from a secret when deployed to kubernetes. Build and push the image to a docker registry. docker build -t ibmcase/mm2ocp:v0.0.2 . docker push ibmcase/mm2ocp:v0.0.2 Then using a deployment configuration like this one , we can deploy our custom mirror maker 2 with: oc apply -f mm2-deployment.yaml # to assess the cluster oc get kafkamirrormaker2 NAME DESIRED REPLICAS mm2-cluster 1 Define the monitoring rules As explained in the monitoring note , we need to define the Prometheus rules within a yaml file so that Mirror Maker 2 can report metrics: lowercaseOutputName : true lowercaseOutputLabelNames : true rules : - pattern : \"kafka.connect<type=connect-worker-metrics>([^:]+):\" name : \"kafka_connect_connect_worker_metrics_$1\" - pattern : \"kafka.connect<type=connect-metrics, client-id=([^:]+)><>([^:]+)\" name : \"kafka_connect_connect_metrics_$1_$2\" # Rules below match the Kafka Connect/MirrorMaker MBeans in the jconsole order # Worker task states - pattern : kafka.connect<type=connect-worker-metrics, connector=(\\w+)><>(connector-destroyed-task-count|connector-failed-task-count|connector-paused-task-count|connector-running-task-count|connector-total-task-count|connector-unassigned-task-count) name : connect_worker_metrics_$1_$2 # Task metrics - pattern : kafka.connect<type=connector-task-metrics, connector=(\\w+), task=(\\d+)><>(batch-size-avg|batch-size-max|offset-commit-avg-time-ms|offset-commit-failure-percentage|offset-commit-max-time-ms|offset-commit-success-percentage|running-ratio) name : connect_connector_task_metrics_$1_$3 labels : task : \"$2\" # Source task metrics - pattern : kafka.connect<type=source-task-metrics, connector=(\\w+), task=(\\d+)><>(source-record-active-count|source-record-poll-total|source-record-write-total) name : connect_source_task_metrics_$1_$3 labels : task : \"$2\" # Task errors - pattern : kafka.connect<type=task-error-metrics, connector=(\\w+), task=(\\d+)><>(total-record-errors|total-record-failures|total-records-skipped|total-retries) name : connect_task_error_metrics_$1_$3 labels : task : \"$2\" # CheckpointConnector metrics - pattern : kafka.connect.mirror<type=MirrorCheckpointConnector, source=(.+), target=(.+), group=(.+), topic=(.+), partition=(\\d+)><>(checkpoint-latency-ms) name : connect_mirror_mirrorcheckpointconnector_$6 labels : source : \"$1\" target : \"$2\" group : \"$3\" topic : \"$4\" partition : \"$5\" # SourceConnector metrics - pattern : kafka.connect.mirror<type=MirrorSourceConnector, target=(.+), topic=(.+), partition=(\\d+)><>(byte-rate|byte-count|record-age-ms|record-rate|record-count|replication-latency-ms) name : connect_mirror_mirrorsourceconnector_$4 labels : target : \"$1\" topic : \"$2\" partition : \"$3\" Then upload this yaml file in a secret (the following command, represents a trick to update an existing configmap) oc create secret generic mm2-jmx-exporter --from-file = ./mm2-jmx-exporter.yaml Deploying on VM On virtual machine, it is possible to deploy the Apache Kafka 2.4+ binary file and then use the command /opt/kafka/bin/connect-mirror-maker.sh with the good properties file as argument. Within a VM we can run multiple mirror maker instances. When needed we can add more VMs to scale horizontally. Each mirror makers workers are part of the same consumer groups, so it is possible to scale at the limit of the topic partition number. Deploying Mirror Maker 2 on its own project In this section we address another approach to, deploy a Kafka Connect cluster with Mirror Maker 2.0 connectors but without any local Kafka Cluster. The approach may be used with Event Streams on Cloud as backend Kafka cluster and Mirror Maker 2 for replication. Using the Strimzi operator we need to define a Yaml file for the connector and white and black lists for the topics to replicate. Here is an example of such descriptor . If we need to run a custom Mirror Maker 2, we have documented in the section above on how to use Dockerfile and properties file and deployment descriptor to do the deployment on kubernetes or OpenShift cluster. Provisioning automation For IT operation automation we can use Ansible to define a playbook to provision the Mirror Maker 2 environment. The Strimzi Ansible playbook repository containts playbook examples for creating cluster roles and service accounts and deploy operators. The automation approach will include: Deploy all cluster objects needed into a OpenShift cluster: Cluster Roles, Strimzi CRDs: Kafka, KafkaTopic, KafkaUser, Kafka connect, mirror maker(s). Deploy all namespaced objects needed into an OpenShift namespace: Service Accounts, Cluster Role Bindings and Role Bindings, Cluster Operator deployment. Deploy the kafka cluster if needed. Deploy the different mirror maker 2 instances. Typical errors in Mirror Maker 2 traces Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. This error message is a light issue in kafka 2.4 and does not impact the replication. In Kafka 2.5 this message is for DEBUG logs. Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}: Those messages may come from multiple reasons. One is that the named topic is not created. In Event Streams is the target cluster the topics may need to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election. The advertised listener may not be set or found. Exception on not being able to create Log directory: do the following: export LOG_DIR=/tmp/logs ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114) Some useful commands Connect to local cluster: oc exec -ti eda-demo-24-cluster-kafka-0 bash list the topics: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list Get the description of the topics from one cluster: for t in $(./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --describe --topic $t done Create MM2 topics manually Here are some examples of command to create topic to the target cluster If you want to delete the topic on your local cluster for t in $(/opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --delete --topic $t done To create the topics manually on the target cluster:exit /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --topic mm2-offset-syncs.kafka-on-premise-cluster-source.internal /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --replication-factor 3 --topic mirrormaker2-cluster-status /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 25 --replication-factor 3 --topic mirrormaker2-cluster-offsets /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic mirrormaker2-cluster-configs /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic heartbeats /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 1 --topic event-streams-wdc.checkpoints.internal","title":"Mirror Maker 2 Provisioning"},{"location":"mm2-provisioning/#mirror-maker-2-deployment","text":"In this article we are presenting different type of Mirror Maker 2 deployments. Updated 4/4 on Strimzi version 0.17. Using Strimzi operator to deploy on Kubernetes To run MM2 on a VM or as docker image which can be adapted with your own configuration, like for example by adding prometheus JMX Exporter as java agent. We are using the configuration to deploy from event streams on Cloud to a local Kafka cluster we deployed using Strimzi.","title":"Mirror Maker 2 Deployment"},{"location":"mm2-provisioning/#common-configuration","text":"When we need to create Kubernetes Secrets to manage APIKEY to access Event Streams, and TLS certificate to access local Kafka brokers, we need to do the following steps: Create a project in OpenShift to deploy Mirror Maker cluster, for example: oc new-project <projectname> . Create a secret for the API KEY of the Event Streams cluster: oc create secret generic es-api-secret --from-literal=password=<replace-with-event-streams-apikey> As your vanilla Kafka source cluster may use TLS to communicate between clients and brokers, you need to use the k8s secret defined when deploying Kafka which includes the CAroot and generic client certificates. These secrets are : eda-demo-24-cluster-clients-ca-cert and eda-demo-24-cluster-cluster-ca-cert . Get the host ip address from the Route resource oc get routes my-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS CA root certificate from the broker oc get secrets oc extract secret/eda-demo-24-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt oc extract secret/eda-demo-24-cluster-clients-ca-cert --keys = ca.crt --to = - >> ca.crt Transform the certificates for java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt Create a secret from truststore file so it can be mounted as needed into consumer or producer running in the same OpenShift cluster. oc create secret generic kafka-truststore --from-file = ./truststore.jks","title":"Common configuration"},{"location":"mm2-provisioning/#deploying-using-strimzi-mirror-maker-operator","text":"We assume you have an existing namespace or project to deploy Mirror Maker. You also need to get the latest (0.17-rc4 at least) Strimzi configuration from the download page . If you have already installed Strimzi Operators, Cluster Roles, and CRDs, you do not need to do it again as those resources are defined at the kubernetes cluster level. See the provisioning note. Define source and target cluster properties in mirror maker 2.0 es-to-kafka-mm2.yml descriptor file. Here is the file for the replication between Event Streams and local cluster es-to-kafka-mm2.yml . We strongly recommend to study the schema definition of this custom resource from this page . Note connectCluster attribute defines the cluster alias used for Kafka Connect, it must match a cluster in the list at spec.clusters . The config part can match the Kafka configuration for consumer or producer, except properties starting by ssl, sasl, security, listeners, rest, bootstarp.servers which are declared at the cluster definition level. alias : \"event-streams-wdc-as-target\" bootstrapServers : broker-3... tls : {} authentication : passwordSecret : secretName : es-api-secret password : password username : token type : plain The example above use a secret to get the Event Streams API KEY, which as create with a command like: oc create secret generic es-api-secret --from-literal=password=<replace with ES key> Deploy Mirror maker 2.0 within your project. oc apply -f es-to-kafka-mm2.yml This commmand creates a kubernetes deployment as illustrated below, with one pod as the replicas is set to 1. If we need to add parallel processing because of the topics to replicate have multiple partitions, or there are a lot of topics to replicate, then adding pods will help to scale horizontally. The pods are in the same consumer group, so Kafka Brokers will do the partition rebalancing among those new added consumers. Now with this deployment we can test consumer and producer as described in the scenario 4 .","title":"Deploying using Strimzi Mirror Maker operator"},{"location":"mm2-provisioning/#mm2-topology","text":"In this section we want to address horizontal scalability and how to organize the MirrorMaker 2 topology for multi tenancy. The simplest approach is to use one MirrorMaker instance per familly of topics: the classification of familly of topic can be anything, from line of business, to team, to application. Suppose an application is using 1000 topic - partitions, for data replication it may make sense to have one MM2 instance for this application responsible to manage the topics replication. The configuration will define the groupId to match the application name for example. The following diagram illustrates this kind of topology by using regular expression on the topic white list selection, there are three MirrorMaker 2 instances mirroring the different topics with name starting with topic-name-A*, topic-name-B*, topic-name-C*, respectively. Each connect instance is a JVM workers that replicate the topic/parititions and has different group.id. For Bi-directional replication for the same topic name, MirrorMaker 2 will use the cluster name as prefix. The following example is showing the configuration for one MM2 connector: apiVersion : kafka . strimzi . io / v1alpha1 kind : KafkaMirrorMaker2","title":"MM2 topology"},{"location":"mm2-provisioning/#capacity-planning","text":"To address capacity planning, we need to review some characteristic of the Kafka Connect framework: For each topic/partition there will be a task running. We can see in the trace that tasks are mapped to threads inside the JVM. So the parallelism will be bound by the number of CPUs the JVM runs on. The parameters max.tasks specifies the max parallel processing we can have per JVM. So for each Topic we need to assess the number of partitions to be replicated. Each task is using the consumer API and is part of the same consumer group, the partition within a group are balanced by an internal controler. With Kafka connect any changes to the topic topology triggers a partition rebalancing. In MM2 each consumer / task is assigned a partition by the controller. So the rebalancing is done internally. Still adding a broker node into the cluster will generate rebalancing. The task processing is stateless, consume - produce wait for acknowledge, commit offet. In this case the CPU and network are key. For platform tuning activity we need to monitor operating system performance metrics. If the CPU becomes the bottleneck, we can allocate more CPU or start to scale horizontally by adding mirror maker 2 instance. If the network at the server level is the bottleneck, then adding more servers will help. Kafka will automatically balance the load among all the tasks running on all the machines. The size of the message impacts also the throughtput as with small message the throughput is CPU bounded. With 100 bytes messages or more we can observe network saturation. The parameters to consider for sizing are the following: Parameter Description Impact Number of topic/ partition Each task processes one partition For pure parallel processing max.tasks is the number of CPU Record size Size of the message in each partition in average Memory usage and Throughput: the # of records/s descrease when size increase, while MB/s throughput increases in logarithmic Expected input throughput The producer writing to the source topic throughput Be sure the consumers inside MM2 absorb the demand Network latency This is where positioning MM2 close to the target cluster may help improve latency","title":"Capacity planning"},{"location":"mm2-provisioning/#version-migration","text":"Once the MirrorMaker cluster is up and running, it may be needed to update the underlying code when a new product version is released. Based on Kafka Connect distributed mode multiple workers JVM coordinate the topic / partition repartition among themselves via Kafka topic. If a worker process dies, the cluster is rebalanced to distribute the work fairly over the remaining workers. If a new worker starts work, a rebalance ensures it takes over some work from the existing workers. Using the REST API it is possible to stop and restart a connector. As of now the recommendation is to start a new MirrorMaker instance with the new version and the same groupId as the existing workers you want to migrate. Then stop the existing version. As each MirrorMaker workers are part of the same group, the internal worker controller will coordinate with the other workers the 'consumer' task to partition assignment. When using Strimzi, if the update applies to the MM2 Custom Resource Definition, just reapplying the CRD should be enough. Be sure to verify the product documentation as new version may enforce to have new topics. It was the case when Kafka connect added the config topic in a recent version.","title":"Version migration"},{"location":"mm2-provisioning/#deploying-a-custom-mirrormaker-docker-image","text":"We want to use custom docker image when we want to add Prometheus JMX exporter as Java Agent so we can monitor MM2 with Prometheus. The proposed docker file is in this folder and may look like: FROM strimzi/kafka:latest-kafka-2.4.0 # ... ENV LOG_DIR = /tmp/logs ENV EXTRA_ARGS = \"-javaagent:/usr/local/share/jars/jmx_prometheus_javaagent-0.12.0.jar=9400:/etc/jmx_exporter/jmx_exporter.yaml \" # .... EXPOSE 9400 CMD /opt/kafka/bin/connect-mirror-maker.sh /home/mm2.properties As the mirror maker 2 is using properties file, we want to define source and target cluster and the security settings for both clusters. As the goal is to run within the same OpenShift cluster as Kafka, the broker list for the source matches the URL within the broker service: # get the service URL oc describe svc my-cluster-kafka-bootstrap # URL my-cluster-kafka-bootstrap:9092 The target cluster uses the bootstrap servers from the Event Streams Credentials, and the API KEY is defined with the manager role, so mirror maker can create topic dynamically. Properties template file can be seen here: kafka-to-es-mm2 clusters = source, target source.bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap:9092 source.ssl.endpoint.identification.algorithm = target.bootstrap.servers = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 target.security.protocol = SASL_SSL target.ssl.protocol = TLSv1.2 target.ssl.endpoint.identification.algorithm = https target.sasl.mechanism = PLAIN target.sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username = \"token\" password=\"<Manager API KEY from Event Streams>\"; # enable and configure individual replication flows source->target.enabled = true sync.topic.acls.enabled = false replication.factor = 3 internal.topic.replication.factor = 3 refresh.topics.interval.seconds = 10 refresh.groups.interval.seconds = 10 source->target.topics = products tasks.max = 10 Upload the properties as a secret oc create secret generic mm2-std-properties --from-file = es-cluster/mm2.properties The file could be copied inside the docker image or better mounted from a secret when deployed to kubernetes. Build and push the image to a docker registry. docker build -t ibmcase/mm2ocp:v0.0.2 . docker push ibmcase/mm2ocp:v0.0.2 Then using a deployment configuration like this one , we can deploy our custom mirror maker 2 with: oc apply -f mm2-deployment.yaml # to assess the cluster oc get kafkamirrormaker2 NAME DESIRED REPLICAS mm2-cluster 1","title":"Deploying a custom MirrorMaker docker image"},{"location":"mm2-provisioning/#define-the-monitoring-rules","text":"As explained in the monitoring note , we need to define the Prometheus rules within a yaml file so that Mirror Maker 2 can report metrics: lowercaseOutputName : true lowercaseOutputLabelNames : true rules : - pattern : \"kafka.connect<type=connect-worker-metrics>([^:]+):\" name : \"kafka_connect_connect_worker_metrics_$1\" - pattern : \"kafka.connect<type=connect-metrics, client-id=([^:]+)><>([^:]+)\" name : \"kafka_connect_connect_metrics_$1_$2\" # Rules below match the Kafka Connect/MirrorMaker MBeans in the jconsole order # Worker task states - pattern : kafka.connect<type=connect-worker-metrics, connector=(\\w+)><>(connector-destroyed-task-count|connector-failed-task-count|connector-paused-task-count|connector-running-task-count|connector-total-task-count|connector-unassigned-task-count) name : connect_worker_metrics_$1_$2 # Task metrics - pattern : kafka.connect<type=connector-task-metrics, connector=(\\w+), task=(\\d+)><>(batch-size-avg|batch-size-max|offset-commit-avg-time-ms|offset-commit-failure-percentage|offset-commit-max-time-ms|offset-commit-success-percentage|running-ratio) name : connect_connector_task_metrics_$1_$3 labels : task : \"$2\" # Source task metrics - pattern : kafka.connect<type=source-task-metrics, connector=(\\w+), task=(\\d+)><>(source-record-active-count|source-record-poll-total|source-record-write-total) name : connect_source_task_metrics_$1_$3 labels : task : \"$2\" # Task errors - pattern : kafka.connect<type=task-error-metrics, connector=(\\w+), task=(\\d+)><>(total-record-errors|total-record-failures|total-records-skipped|total-retries) name : connect_task_error_metrics_$1_$3 labels : task : \"$2\" # CheckpointConnector metrics - pattern : kafka.connect.mirror<type=MirrorCheckpointConnector, source=(.+), target=(.+), group=(.+), topic=(.+), partition=(\\d+)><>(checkpoint-latency-ms) name : connect_mirror_mirrorcheckpointconnector_$6 labels : source : \"$1\" target : \"$2\" group : \"$3\" topic : \"$4\" partition : \"$5\" # SourceConnector metrics - pattern : kafka.connect.mirror<type=MirrorSourceConnector, target=(.+), topic=(.+), partition=(\\d+)><>(byte-rate|byte-count|record-age-ms|record-rate|record-count|replication-latency-ms) name : connect_mirror_mirrorsourceconnector_$4 labels : target : \"$1\" topic : \"$2\" partition : \"$3\" Then upload this yaml file in a secret (the following command, represents a trick to update an existing configmap) oc create secret generic mm2-jmx-exporter --from-file = ./mm2-jmx-exporter.yaml","title":"Define the monitoring rules"},{"location":"mm2-provisioning/#deploying-on-vm","text":"On virtual machine, it is possible to deploy the Apache Kafka 2.4+ binary file and then use the command /opt/kafka/bin/connect-mirror-maker.sh with the good properties file as argument. Within a VM we can run multiple mirror maker instances. When needed we can add more VMs to scale horizontally. Each mirror makers workers are part of the same consumer groups, so it is possible to scale at the limit of the topic partition number.","title":"Deploying on VM"},{"location":"mm2-provisioning/#deploying-mirror-maker-2-on-its-own-project","text":"In this section we address another approach to, deploy a Kafka Connect cluster with Mirror Maker 2.0 connectors but without any local Kafka Cluster. The approach may be used with Event Streams on Cloud as backend Kafka cluster and Mirror Maker 2 for replication. Using the Strimzi operator we need to define a Yaml file for the connector and white and black lists for the topics to replicate. Here is an example of such descriptor . If we need to run a custom Mirror Maker 2, we have documented in the section above on how to use Dockerfile and properties file and deployment descriptor to do the deployment on kubernetes or OpenShift cluster.","title":"Deploying Mirror Maker 2 on its own project"},{"location":"mm2-provisioning/#provisioning-automation","text":"For IT operation automation we can use Ansible to define a playbook to provision the Mirror Maker 2 environment. The Strimzi Ansible playbook repository containts playbook examples for creating cluster roles and service accounts and deploy operators. The automation approach will include: Deploy all cluster objects needed into a OpenShift cluster: Cluster Roles, Strimzi CRDs: Kafka, KafkaTopic, KafkaUser, Kafka connect, mirror maker(s). Deploy all namespaced objects needed into an OpenShift namespace: Service Accounts, Cluster Role Bindings and Role Bindings, Cluster Operator deployment. Deploy the kafka cluster if needed. Deploy the different mirror maker 2 instances.","title":"Provisioning automation"},{"location":"mm2-provisioning/#typical-errors-in-mirror-maker-2-traces","text":"Plugin class loader for connector: 'org.apache.kafka.connect.mirror.MirrorCheckpointConnector' was not found. This error message is a light issue in kafka 2.4 and does not impact the replication. In Kafka 2.5 this message is for DEBUG logs. Error while fetching metadata with correlation id 2314 : {source.heartbeats=UNKNOWN_TOPIC_OR_PARTITION}: Those messages may come from multiple reasons. One is that the named topic is not created. In Event Streams is the target cluster the topics may need to be created via CLI or User Interface. It can also being related to the fact the consumer polls on a topic that has just been created and the leader for this topic-partition is not yet available, you are in the middle of a leadership election. The advertised listener may not be set or found. Exception on not being able to create Log directory: do the following: export LOG_DIR=/tmp/logs ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to flush, timed out while waiting for producer to flush outstanding 1 messages ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Failed to commit offsets (org.apache.kafka.connect.runtime.SourceTaskOffsetCommitter:114) Some useful commands Connect to local cluster: oc exec -ti eda-demo-24-cluster-kafka-0 bash list the topics: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list Get the description of the topics from one cluster: for t in $(./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --describe --topic $t done","title":"Typical errors in Mirror Maker 2 traces"},{"location":"mm2-provisioning/#create-mm2-topics-manually","text":"Here are some examples of command to create topic to the target cluster If you want to delete the topic on your local cluster for t in $(/opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --list) do ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --delete --topic $t done To create the topics manually on the target cluster:exit /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --topic mm2-offset-syncs.kafka-on-premise-cluster-source.internal /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 5 --replication-factor 3 --topic mirrormaker2-cluster-status /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 25 --replication-factor 3 --topic mirrormaker2-cluster-offsets /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic mirrormaker2-cluster-configs /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 3 --topic heartbeats /opt/kafka/bin/kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions 1 --replication-factor 1 --topic event-streams-wdc.checkpoints.internal","title":"Create MM2 topics manually"},{"location":"monitoring/","text":"Monitoring Mirror Maker and kafka connect cluster The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics using Prometheus and present them with Grafana dashboards. Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. The following figure presents the prometheus generic architecture as described from the product main website. Basically the Prometheus server hosts jobs to poll HTTP end points to get metrics from the components to monitor. It supports queries in the format of PromQL , that product like Grafana can use to present nice dashboards, and it can push alerts to different channels when some metrics behave unexpectedly. In the context of data replication between kafka clusters, we want to monitor the mirror maker 2.0 metrics like the worker task states, source task metrics, task errors,... The following figure illustrates the components involved: The source Kafka cluster, the Mirror Maker 2.0 cluster, which is based on Kafka Connect, the Prometheus server and the Grafana. As all those components run on kubernetes, most of them could be deployed via Operators using Custom Resource Definitions. To support this monitoring we need to do the following steps: Add metrics configuration to your Mirror Maker 2.0 cluster Package the mirror maker 2 to use JMX Exporter as Java agent so it exposes JMX MBeans as metrics accessibles via HTTP. Deploy Prometheus using Operator Optionally deploy Prometheus Alertmanager Deploy Grafana and configure dashboard Installation and configuration Prometheus deployment inside Kubernetes uses operator as defined in the coreos github . The CRDs define a set of resources: the ServiceMonitor, PodMonitor, and PrometheusRule. Inside the Strimzi github repository , we can get a prometheus.yml file to deploy prometheus server using the Prometheus operator . This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance. We have defined our own configuration in this file . For your own deployment you have to change the target namespace, and the rules You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the Mirror Maker 2 Cluster. To be able to monitor your own on-premise Kafka cluster, you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment with Prometheus setting can be found in our kafka cluster definition . The declarations are under the metrics stanza and define the rules for exposing the Kafka core features. Install Prometheus Operator We recommend reading Prometheus operator product documentation. At a glance the Prometheus operator deploy and manage a prometheus server and watches new pods to monitor when they are scheduled within k8s. Source: prometheus-operator architecture After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator and the related service account, cluster role, role binding... We have reuse the monitoring/install/bundle.yaml from Prometheus operator github, but doing updates with a namespace sets for our project (e.g jb-kafka-strimzi ) and renaming the cluster role and binding from 'prometheus-operator' to 'prometheus-operator-strimzi` to avoid role conflict with existing prometheus deployment on OpenShift, as those roles are at the cluster level. Once done we deploy all those components: oc apply -f bundle.yaml # Authorise the prometheus-operator to do cluster work oc adm policy add-cluster-role-to-user prometheus-operator --serviceaccount prometheus-operator -n eda-strimzi-kafka24 When you apply those configurations, the following resources are visibles: Resource Description ClusterRole RBAC role for cluster-scoped resources. To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. A service account provides an identity for processes that run in a Pod. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To define the service to monitor with the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource) To delete the operator do: oc delete -f bundle.yaml Deploy prometheus Note The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker 2 or Kafka Connect monitoring, the configuration will have less rules, and parameters. See next section . Deploy the prometheus server by first changing the namespace and also by adapting the original examples/metrics/prometheus-install/prometheus.yaml file . curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: eda-strimzi-kafka24/\" > prometheus.yml If you are using AlertManager (see section below ) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-rules.yaml oc apply -f prometheus-rules.yaml oc apply -f prometheus.yaml # once deploye, get the state of the server with oc get prometheus NAME VERSION REPLICAS AGE prometheus 1 52s The Prometheus server configuration uses service discovery to discover the pods (Mirror Maker 2.0 pod or kafka, zookeeper pods) in the cluster from which it gets metrics. In fact the following configuration is set in prometheus.yaml file. The approach is to deploy one Prometheus server instance per namespace where multiple applications are running. The app label needs to be set on all components to be monitored. serviceMonitorSelector : matchLabels : app : strimzi or use a monitor all approach: serviceMonitorSelector : {} Monitoring rules can be added via config map that is referenced in the prometheus.yaml file: additionalScrapeConfigs : name : additional-scrape-configs key : prometheus-additional.yaml Access the expression browser To access from web browser we can expose the prometheus server via a route using the service operator defined in the prometheus.yaml file: apiVersion : v1 kind : Service metadata : labels : prometheus : prometheus name : prometheus-operated namespace : eda-strimzi-kafka24 spec : ports : - name : web port : 9090 targetPort : web selector : app : strimzi prometheus : prometheus sessionAffinity : ClientIP Once define the url will be something like: http://prometheus-route-eda-strimzi-kafka24.gse-eda-demo-43-f......us-east.containers.appdomain.cloud/graph Configure monitoring To start monitoring our Kafka 2.4 cluster we need to add some monitoring prometheus scrapper definitions, named service monitor. An example of such file can be found here oc apply -f strimzi-service-monitor.yaml oc describe servicemonitor Mirror maker 2.0 monitoring To monitor MM2 with Prometheus we need to add JMX Exporter and run it as Java agent.The jar file for JMX exporter agent can be found here . We copied a version in the folder mirror-maker-2/libs . We have adopted a custom mirror maker 2.0 docker imaged based on Kafka 2.4. We are detailing how to build this image using this Dockerfile in this separate note . The next step is to define a service monitor Once the Mirror Maker 2.0 is connected... Install Grafana Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker. Deploy Grafan to OpenShift and expose it via a service: oc apply -f grafana.yaml In case you want to test grafana locally run: docker run -d -p 3000:3000 grafana/grafana Kafka Explorer Configure Grafana dashboard To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service. Use port forwarding: export PODNAME = $( oc get pods -l name = grafana | grep grafana | awk '{print $1}' ) kubectl port-forward $PODNAME 3000 :3000 Point your browser to http://localhost:3000 . Expose the route via cli Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090 Alert Manager As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file The prometheus-rules.yaml . Those rules are used by the AlertManager component. Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager. Download an example of alert manager configuration file curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml > alert-manager.yaml Define a configuration for the channel to use, by starting from the following template curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml > alert-manager-config.yaml Modify this file to reflect the remote access credential and URL to the channel server. Then deploy the secret that matches your config file . oc create secret generic alertmanager-alertmanager --from-file = alertmanager.yaml = alert-manager-config.yaml oc create secret generic additional-scrape-configs --from-file = ./local-cluster/prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f - Further Readings Monitoring Event Streams cluster health with Prometheus How to monitor applications on OpenShift 4.x with Prometheus Operator Access Event streams using secured JMX connections","title":"Monitoring with Prometheus and Grafana"},{"location":"monitoring/#monitoring-mirror-maker-and-kafka-connect-cluster","text":"The goal of this note is to go over some of the details on how to monitor Mirror Maker 2.0 metrics using Prometheus and present them with Grafana dashboards. Prometheus is an open source systems monitoring and alerting toolkit that, with Kubernetes, is part of the Cloud Native Computing Foundation. It can monitor multiple workloads but is normally used with container workloads. The following figure presents the prometheus generic architecture as described from the product main website. Basically the Prometheus server hosts jobs to poll HTTP end points to get metrics from the components to monitor. It supports queries in the format of PromQL , that product like Grafana can use to present nice dashboards, and it can push alerts to different channels when some metrics behave unexpectedly. In the context of data replication between kafka clusters, we want to monitor the mirror maker 2.0 metrics like the worker task states, source task metrics, task errors,... The following figure illustrates the components involved: The source Kafka cluster, the Mirror Maker 2.0 cluster, which is based on Kafka Connect, the Prometheus server and the Grafana. As all those components run on kubernetes, most of them could be deployed via Operators using Custom Resource Definitions. To support this monitoring we need to do the following steps: Add metrics configuration to your Mirror Maker 2.0 cluster Package the mirror maker 2 to use JMX Exporter as Java agent so it exposes JMX MBeans as metrics accessibles via HTTP. Deploy Prometheus using Operator Optionally deploy Prometheus Alertmanager Deploy Grafana and configure dashboard","title":"Monitoring Mirror Maker and kafka connect cluster"},{"location":"monitoring/#installation-and-configuration","text":"Prometheus deployment inside Kubernetes uses operator as defined in the coreos github . The CRDs define a set of resources: the ServiceMonitor, PodMonitor, and PrometheusRule. Inside the Strimzi github repository , we can get a prometheus.yml file to deploy prometheus server using the Prometheus operator . This configuration defines, ClusterRole, ServiceAccount, ClusterRoleBinding, and the Prometheus resource instance. We have defined our own configuration in this file . For your own deployment you have to change the target namespace, and the rules You need to deploy Prometheus and all the other elements inside the same namespace or OpenShift project as the Kafka Cluster or the Mirror Maker 2 Cluster. To be able to monitor your own on-premise Kafka cluster, you need to enable Prometheus metrics. An example of Kafka cluster Strimzi based deployment with Prometheus setting can be found in our kafka cluster definition . The declarations are under the metrics stanza and define the rules for exposing the Kafka core features.","title":"Installation and configuration"},{"location":"monitoring/#install-prometheus-operator","text":"We recommend reading Prometheus operator product documentation. At a glance the Prometheus operator deploy and manage a prometheus server and watches new pods to monitor when they are scheduled within k8s.","title":"Install Prometheus Operator"},{"location":"monitoring/#source-prometheus-operator-architecture","text":"After creating a namespace or reusing the Kafka cluster namespace, you need to deploy the Prometheus operator and the related service account, cluster role, role binding... We have reuse the monitoring/install/bundle.yaml from Prometheus operator github, but doing updates with a namespace sets for our project (e.g jb-kafka-strimzi ) and renaming the cluster role and binding from 'prometheus-operator' to 'prometheus-operator-strimzi` to avoid role conflict with existing prometheus deployment on OpenShift, as those roles are at the cluster level. Once done we deploy all those components: oc apply -f bundle.yaml # Authorise the prometheus-operator to do cluster work oc adm policy add-cluster-role-to-user prometheus-operator --serviceaccount prometheus-operator -n eda-strimzi-kafka24 When you apply those configurations, the following resources are visibles: Resource Description ClusterRole RBAC role for cluster-scoped resources. To grant permissions to Prometheus to read the health endpoints exposed by the Kafka and ZooKeeper pods, cAdvisor and the kubelet for container metrics. ServiceAccount For the Prometheus pods to run under. A service account provides an identity for processes that run in a Pod. ClusterRoleBinding To bind the ClusterRole to the ServiceAccount. Deployment To manage the Prometheus Operator pod. ServiceMonitor To define the service to monitor with the Prometheus pod. Prometheus To manage the configuration of the Prometheus pod. PrometheusRule To manage alerting rules for the Prometheus pod. Secret To manage additional Prometheus settings. Service To allow applications running in the cluster to connect to Prometheus (for example, Grafana using Prometheus as datasource) To delete the operator do: oc delete -f bundle.yaml","title":"Source: prometheus-operator architecture"},{"location":"monitoring/#deploy-prometheus","text":"Note The following section is including the configuration of a Prometheus server monitoring a full Kafka Cluster. For Mirror Maker 2 or Kafka Connect monitoring, the configuration will have less rules, and parameters. See next section . Deploy the prometheus server by first changing the namespace and also by adapting the original examples/metrics/prometheus-install/prometheus.yaml file . curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus.yaml | sed -e \"s/namespace: myproject/namespace: eda-strimzi-kafka24/\" > prometheus.yml If you are using AlertManager (see section below ) Define the monitoring rules of the kafka run time: KafkaRunningOutOfSpace, UnderReplicatedPartitions, AbnormalControllerState, OfflinePartitions, UnderMinIsrPartitionCount, OfflineLogDirectoryCount, ScrapeProblem (Prometheus related alert), ClusterOperatorContainerDown, KafkaBrokerContainersDown, KafkaTlsSidecarContainersDown curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/prometheus-rules.yaml sed -e \"s/namespace: default/namespace: jb-kafka-strimzi/\" > prometheus-rules.yaml oc apply -f prometheus-rules.yaml oc apply -f prometheus.yaml # once deploye, get the state of the server with oc get prometheus NAME VERSION REPLICAS AGE prometheus 1 52s The Prometheus server configuration uses service discovery to discover the pods (Mirror Maker 2.0 pod or kafka, zookeeper pods) in the cluster from which it gets metrics. In fact the following configuration is set in prometheus.yaml file. The approach is to deploy one Prometheus server instance per namespace where multiple applications are running. The app label needs to be set on all components to be monitored. serviceMonitorSelector : matchLabels : app : strimzi or use a monitor all approach: serviceMonitorSelector : {} Monitoring rules can be added via config map that is referenced in the prometheus.yaml file: additionalScrapeConfigs : name : additional-scrape-configs key : prometheus-additional.yaml","title":"Deploy prometheus"},{"location":"monitoring/#access-the-expression-browser","text":"To access from web browser we can expose the prometheus server via a route using the service operator defined in the prometheus.yaml file: apiVersion : v1 kind : Service metadata : labels : prometheus : prometheus name : prometheus-operated namespace : eda-strimzi-kafka24 spec : ports : - name : web port : 9090 targetPort : web selector : app : strimzi prometheus : prometheus sessionAffinity : ClientIP Once define the url will be something like: http://prometheus-route-eda-strimzi-kafka24.gse-eda-demo-43-f......us-east.containers.appdomain.cloud/graph","title":"Access the expression browser"},{"location":"monitoring/#configure-monitoring","text":"To start monitoring our Kafka 2.4 cluster we need to add some monitoring prometheus scrapper definitions, named service monitor. An example of such file can be found here oc apply -f strimzi-service-monitor.yaml oc describe servicemonitor","title":"Configure monitoring"},{"location":"monitoring/#mirror-maker-20-monitoring","text":"To monitor MM2 with Prometheus we need to add JMX Exporter and run it as Java agent.The jar file for JMX exporter agent can be found here . We copied a version in the folder mirror-maker-2/libs . We have adopted a custom mirror maker 2.0 docker imaged based on Kafka 2.4. We are detailing how to build this image using this Dockerfile in this separate note . The next step is to define a service monitor Once the Mirror Maker 2.0 is connected...","title":"Mirror maker 2.0 monitoring"},{"location":"monitoring/#install-grafana","text":"Grafana provides visualizations of Prometheus metrics. Again we will use the Strimzi dashboard definition as starting point to monitor Kafka cluster but also mirror maker. Deploy Grafan to OpenShift and expose it via a service: oc apply -f grafana.yaml In case you want to test grafana locally run: docker run -d -p 3000:3000 grafana/grafana","title":"Install Grafana"},{"location":"monitoring/#kafka-explorer","text":"","title":"Kafka Explorer"},{"location":"monitoring/#configure-grafana-dashboard","text":"To access the Grafana portal you can use port forwarding like below or expose a route on top of the grafana service. Use port forwarding: export PODNAME = $( oc get pods -l name = grafana | grep grafana | awk '{print $1}' ) kubectl port-forward $PODNAME 3000 :3000 Point your browser to http://localhost:3000 . Expose the route via cli Add the Prometheus data source with the URL of the exposed routes. http://prometheus-operated:9090","title":"Configure Grafana dashboard"},{"location":"monitoring/#alert-manager","text":"As seen in previous section, when deploying prometheus we can set some alerting rules on elements of the Kafka cluster. Those rule examples are in the file The prometheus-rules.yaml . Those rules are used by the AlertManager component. Prometheus Alertmanager is a plugin for handling alerts and routing them to a notification service, like Slack. The Prometheus server is a client to the Alert Manager. Download an example of alert manager configuration file curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-install/alert-manager.yaml > alert-manager.yaml Define a configuration for the channel to use, by starting from the following template curl -s https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/master/examples/metrics/prometheus-alertmanager-config/alert-manager-config.yaml > alert-manager-config.yaml Modify this file to reflect the remote access credential and URL to the channel server. Then deploy the secret that matches your config file . oc create secret generic alertmanager-alertmanager --from-file = alertmanager.yaml = alert-manager-config.yaml oc create secret generic additional-scrape-configs --from-file = ./local-cluster/prometheus-additional.yaml --dry-run -o yaml | kubectl apply -f -","title":"Alert Manager"},{"location":"monitoring/#further-readings","text":"Monitoring Event Streams cluster health with Prometheus How to monitor applications on OpenShift 4.x with Prometheus Operator Access Event streams using secured JMX connections","title":"Further Readings"},{"location":"perf-tests/","text":"Validation and Performance tests The mirroring validation and performance tests are based on the architecture depicted in the following figure, where MirrorMaker 2 is running close to the on-premise target cluster and the source is Event Streams on Cloud. Validate the topic replication Objective Determine if MirrorMaker 2 can successfully replicate messages from a topic with n partitions on the source Kakfa cluster to a topic with n-m partitions on the target Kafka cluster. Given If given Manager rights on Event Streams, where Event Streams is the target Kafka cluster, MirrorMaker 2 will either create the target topic with n partitions or increase the number of partitions on the topic to n. Both of these default behaviours are proven. Method 1 Create new Credential on IES on Cloud with Write permission, but not Manager permissions. Reconfigure MirrorMaker 2 with the new Credential. Create a new topic on IES on Cloud with 5 partitions that corresponds to a new topic on local Kafka with 10 partitions. Test replication from the local Kafka topic to the topic on IES on IBM Cloud. Observe any errors in MirrorMaker 2 log, and validate data replicates as expected. Hypothesis Data will replicate successfully. The MirrorMaker 2 architecture uses Kafka Connect, and so simply acts as a 3 rd party consumer of topic data on the source side and writes to the topic on the target side as any other producer would. Given sufficient partitions on each of the source and target topics for MirrorMaker 2 to act as a consumer (on the source side) and a producer (on the target side), how many total partitions each side has should not be a factor. Method 2 Create new Credential on IES on Cloud with Write permission, but not Manager permissions. Reconfigure MirrorMaker 2 with the new Credential. Create a new topic on IES on Cloud with 10 partitions that corresponds to a new topic on local Kafka with 5 partitions. Test replication from the local Kafka topic to the topic on IES on IBM Cloud. Observe any errors in MirrorMaker 2 log, and validate data replicates as expected. Hypothesis (Same as previous hypothesis) Data will replicate successfully. The MirrorMaker 2 architecture uses Kafka Connect, and so simply acts as a 3 rd party consumer of topic data on the source side and writes to the topic on the target side as any other producer would. Given sufficient partitions on each of the source and target topics for MirrorMaker 2 to act as a consumer (on the source side) and a producer (on the target side), how many total partitions each side has should not be a factor. Method 1 - Implementation Get API KEYS and BROKERS URL for Event Streams { \"api_key\" : \"_V...d\" , \"apikey\" : \"_VX...d\" , \"iam_apikey_description\" : \"Auto-generated for key ab427...6140e186969\" , \"iam_apikey_name\" : \"kp-writer\" , \"instance_id\" : \"7.....\" , \"kafka_admin_url\" : \"https://2ym....krl.svc03.us-south.eventstreams.cloud.ibm.com\" , \"kafka_brokers_sasl\" : [ \"broker-3-2ymhj.....kafka.svc03.us-south.eventstreams.cloud.ibm.com:9093\" , \"broker-4-2ymhj....kafka.svc03.us-south.eventstreams.cloud.ibm.com:9093\" , \"broker-1-2ymhjg....kafka.svc03.us-south.eventstreams.cloud.ibm.com:9093\" ], \"kafka_http_url\" : \"https://2ym....krl.svc03.us-south.eventstreams.cloud.ibm.com\" , \"password\" : \"_...\" , \"user\" : \"token\" } Create a topic with 10 partitions on source cluster (e.g. kp-topic-1) It could be done using CLI like: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions = 10 --topic kp-topic-1 Create a topic with 5 partitions on target cluster (e.g. kp-local.kp-topic-1) If you need to look at the topic information using the shell command use something like: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --describe --topic source.accounts Topic: source.accounts PartitionCount: 5 ReplicationFactor: 3 Configs: message.format.version = 2 .4-IV1 Topic: source.accounts Partition: 0 Leader: 0 Replicas: 0 ,2,1 Isr: 0 ,2,1 Topic: source.accounts Partition: 1 Leader: 2 Replicas: 2 ,1,0 Isr: 2 ,1,0 Topic: source.accounts Partition: 2 Leader: 1 Replicas: 1 ,0,2 Isr: 1 ,0,2 Topic: source.accounts Partition: 3 Leader: 0 Replicas: 0 ,1,2 Isr: 0 ,1,2 Topic: source.accounts Partition: 4 Leader: 2 Replicas: 2 ,0,1 Isr: 2 ,0,1 Create a MirrorMaker 2 configuration to replicate the topic Python based test approach This test is in the perf-tests/ValidateTopicReplication folder, and aims to validate the 10 partition topic to 5 partition topic replication. Send 500 records to the source topic using the producer code: ProducerPerformance.py Start the consumer on target topic in a unique consumer group so it gets the messages from all the 5 partitions, compare the number of message received: it should be 500. The procedure is: export the environment variable to access target cluster: export KAFKA_BROKERS = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY = \"event stream apikey\" Export the environment variables to point to the source cluster: export KAFKA_SRC_CERT = /home/ca.crt export KAFKA_SRC_BROKERS = eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud Start consumer: Start the consumer to run locally but remote connected to the kafka cluster: docker run -ti -v $( pwd ) :/home -e KAFKA_BROKERS = $KAFKA_TGT_BROKERS -e KAFKA_CERT = $KAFKA_TGT_CERT ibmcase/python37 bash -c \"python /home/PerfConsumer.py --topic source.accounts\" Or run the consumer inside openshift oc run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TGT_BROKERS --topic source.accounts --from-beginning Start producer docker run -ti -v $( pwd ) :/home -e KAFKA_BROKERS = $KAFKA_SRC_BROKERS ibmcase/python37 bash -c \"python ProducerPerformance.py --file /home/ValidateTopicReplication/testplayload.json --size 500 --keyname identifier --topic accounts\" The number of records on the consumer side should match the number send (e.g. 500), but this time read from 5 partitions. Conclusion Method 1 Hypothesis proven - MirrorMaker 2 can replicate from a local topic with 10 partitions to a remote topic with 5 partitions. Java based performance tests The performance testing in java is based on two tools: the IBM Event Stream tool which is itself based on the Apache Kafka producer performance tool and a custom consumer app, deployable on kubernetes cluster and that support latency reporting. Context The following diagram illustrates the performance test app context, with the two kafka clusters and the MirrorMaker tool: Zooming into the system, we define the producer and consumer apps and we may add metrics reporting consumable by using dashboard. For the app producer, we have cloned this tool under the perf-test/event-streams-sample-producer folder. The tool reports test metrics like records per second, number of records sent, the megabytes per second, the average and maximum latencies, from the producer.metrics() and other stats from the tools. The arguments supported are: Argument Description --record-size value > or --payload-file filename one is mandatory but not both. Work only for UTF-8 encoded text files --topic name mandatory --num-records value mandatory --payload-delimiter char delimiter to be used when --payload-file is provided. Default is \\n --throughput value Throttle to value messages/sec. -1 to disable throtlling --producer.config filename producer config properties file --producer-props prop_name=value producer configuration properties --print-metrics true|false Default to true --transactional-id value Test with transaction --transaction-duration-ms value The max age of each transaction. Test with transaction if v>0 To completement this tool, the consumer app is responsible to measure some of the latency between given timestamps. The following diagram presents the interesting time stamps we can assess with some tooling: ts-1: timestamp when creating the record object before sending ts-2: record timestamp when broker write to topic-partition: source topic ts-3: record timestamp when broker write to topic-partition: target topic ts-4: timestamp when polling the record To get timestamp at the topic level, we need to add the message.timestamp.type: LogAppendTime property when creating the topic. The consumer needs to be deployable on OpenShift to scale horizontally. The metrics can be exposed as metrics for Prometheus. The metrics are: average latency, min and max latencies. The performance test consumer webapp is under the perf-tests/perf-consumer-app folder . The readme, in this folder, explains how to build and deploy it. Test approach Using IBM event streams producer tool we can run 3 different workload size, the payload is generated with random bytes or with records read from a data file. java -jar target/es-producer.jar -t accounts -s small -c java -jar target/es-producer.jar -t accounts -s medium java -jar target/es-producer.jar -t accounts -s large To build the jar,run mvn package in the event-streams-sample-producer-1.1.0 folder. Here is an example of call to this performance producer java -jar event-streams-sample-producer-1.1.0/target/es-producer.jar --payload-file ./da/records.json -t topic --producer-config ../mirror-maker-2/eventstream.properties The eventstream.properties file define the bootstrap.servers, sasl configuration using Event Streams credentials and URLs. Something like: bootstrap.servers = broker-3-.....eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<replace with APIKAY>\"; The tool reports the following measures: 1261 records sent, 251 .8 records/sec ( 0 .97 MB/sec ) , 2233 .3 ms avg latency, 3320 .0 max latency. 2848 records sent, 569 .5 records/sec ( 2 .19 MB/sec ) , 5797 .0 ms avg latency, 8294 .0 max latency. 2840 records sent, 567 .8 records/sec ( 2 .19 MB/sec ) , 10760 .9 ms avg latency, 13288 .0 max latency. .. 60000 records sent, 490 .869821 records/sec ( 1 .89 MB/sec ) , 14698 .93 ms avg latency, 24949 .00 ms max latency, 14490 ms 50th, 22298 ms 95th, 23413 ms 99th, 24805 ms 99 .9th. Measuring with consumer app The consumer app offers an API to get performance metrics via HTTP or via metrics. The ones interesting are: /perf/config to get the cluster configuration, it should return a json document like { \"ssl.protocol\" : \"TLSv1.2\" , \"sasl.mechanism\" : \"PLAIN\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"client.id\" : \"test-cons-group-client-a425be84-801b-42b5-af2f-340386f98896\" , \"ssl.truststore.password\" : \"password\" , \"ssl.endpoint.identification.algorithm\" : \"HTTPS\" , \"ssl.enabled.protocols\" : \"TLSv1.2\" , \"ssl.truststore.location\" : \"/home/truststore.jks\" , \"bootstrap.servers\" : \"eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud:443\" , \"auto.offset.reset\" : \"earliest\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"group.id\" : \"test-cons-group\" , \"enable.auto.commit\" : \"false\" , \"security.protocol\" : \"SSL\" } /perf/current for getting the current metrics vias HTTP. Here is an example of output","title":"Tests"},{"location":"perf-tests/#validation-and-performance-tests","text":"The mirroring validation and performance tests are based on the architecture depicted in the following figure, where MirrorMaker 2 is running close to the on-premise target cluster and the source is Event Streams on Cloud.","title":"Validation and Performance tests"},{"location":"perf-tests/#validate-the-topic-replication","text":"","title":"Validate the topic replication"},{"location":"perf-tests/#objective","text":"Determine if MirrorMaker 2 can successfully replicate messages from a topic with n partitions on the source Kakfa cluster to a topic with n-m partitions on the target Kafka cluster.","title":"Objective"},{"location":"perf-tests/#given","text":"If given Manager rights on Event Streams, where Event Streams is the target Kafka cluster, MirrorMaker 2 will either create the target topic with n partitions or increase the number of partitions on the topic to n. Both of these default behaviours are proven.","title":"Given"},{"location":"perf-tests/#method-1","text":"Create new Credential on IES on Cloud with Write permission, but not Manager permissions. Reconfigure MirrorMaker 2 with the new Credential. Create a new topic on IES on Cloud with 5 partitions that corresponds to a new topic on local Kafka with 10 partitions. Test replication from the local Kafka topic to the topic on IES on IBM Cloud. Observe any errors in MirrorMaker 2 log, and validate data replicates as expected.","title":"Method 1"},{"location":"perf-tests/#hypothesis","text":"Data will replicate successfully. The MirrorMaker 2 architecture uses Kafka Connect, and so simply acts as a 3 rd party consumer of topic data on the source side and writes to the topic on the target side as any other producer would. Given sufficient partitions on each of the source and target topics for MirrorMaker 2 to act as a consumer (on the source side) and a producer (on the target side), how many total partitions each side has should not be a factor.","title":"Hypothesis"},{"location":"perf-tests/#method-2","text":"Create new Credential on IES on Cloud with Write permission, but not Manager permissions. Reconfigure MirrorMaker 2 with the new Credential. Create a new topic on IES on Cloud with 10 partitions that corresponds to a new topic on local Kafka with 5 partitions. Test replication from the local Kafka topic to the topic on IES on IBM Cloud. Observe any errors in MirrorMaker 2 log, and validate data replicates as expected.","title":"Method 2"},{"location":"perf-tests/#hypothesis_1","text":"(Same as previous hypothesis) Data will replicate successfully. The MirrorMaker 2 architecture uses Kafka Connect, and so simply acts as a 3 rd party consumer of topic data on the source side and writes to the topic on the target side as any other producer would. Given sufficient partitions on each of the source and target topics for MirrorMaker 2 to act as a consumer (on the source side) and a producer (on the target side), how many total partitions each side has should not be a factor.","title":"Hypothesis"},{"location":"perf-tests/#method-1-implementation","text":"Get API KEYS and BROKERS URL for Event Streams { \"api_key\" : \"_V...d\" , \"apikey\" : \"_VX...d\" , \"iam_apikey_description\" : \"Auto-generated for key ab427...6140e186969\" , \"iam_apikey_name\" : \"kp-writer\" , \"instance_id\" : \"7.....\" , \"kafka_admin_url\" : \"https://2ym....krl.svc03.us-south.eventstreams.cloud.ibm.com\" , \"kafka_brokers_sasl\" : [ \"broker-3-2ymhj.....kafka.svc03.us-south.eventstreams.cloud.ibm.com:9093\" , \"broker-4-2ymhj....kafka.svc03.us-south.eventstreams.cloud.ibm.com:9093\" , \"broker-1-2ymhjg....kafka.svc03.us-south.eventstreams.cloud.ibm.com:9093\" ], \"kafka_http_url\" : \"https://2ym....krl.svc03.us-south.eventstreams.cloud.ibm.com\" , \"password\" : \"_...\" , \"user\" : \"token\" } Create a topic with 10 partitions on source cluster (e.g. kp-topic-1) It could be done using CLI like: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --create --partitions = 10 --topic kp-topic-1 Create a topic with 5 partitions on target cluster (e.g. kp-local.kp-topic-1) If you need to look at the topic information using the shell command use something like: ./kafka-topics.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --describe --topic source.accounts Topic: source.accounts PartitionCount: 5 ReplicationFactor: 3 Configs: message.format.version = 2 .4-IV1 Topic: source.accounts Partition: 0 Leader: 0 Replicas: 0 ,2,1 Isr: 0 ,2,1 Topic: source.accounts Partition: 1 Leader: 2 Replicas: 2 ,1,0 Isr: 2 ,1,0 Topic: source.accounts Partition: 2 Leader: 1 Replicas: 1 ,0,2 Isr: 1 ,0,2 Topic: source.accounts Partition: 3 Leader: 0 Replicas: 0 ,1,2 Isr: 0 ,1,2 Topic: source.accounts Partition: 4 Leader: 2 Replicas: 2 ,0,1 Isr: 2 ,0,1 Create a MirrorMaker 2 configuration to replicate the topic","title":"Method 1 - Implementation"},{"location":"perf-tests/#python-based-test-approach","text":"This test is in the perf-tests/ValidateTopicReplication folder, and aims to validate the 10 partition topic to 5 partition topic replication. Send 500 records to the source topic using the producer code: ProducerPerformance.py Start the consumer on target topic in a unique consumer group so it gets the messages from all the 5 partitions, compare the number of message received: it should be 500. The procedure is: export the environment variable to access target cluster: export KAFKA_BROKERS = broker-3-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-1-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-0-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-5-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-2-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093,broker-4-h6s2xk6b2t77g4p1.kafka.svc01.us-east.eventstreams.cloud.ibm.com:9093 export KAFKA_APIKEY = \"event stream apikey\" Export the environment variables to point to the source cluster: export KAFKA_SRC_CERT = /home/ca.crt export KAFKA_SRC_BROKERS = eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud Start consumer: Start the consumer to run locally but remote connected to the kafka cluster: docker run -ti -v $( pwd ) :/home -e KAFKA_BROKERS = $KAFKA_TGT_BROKERS -e KAFKA_CERT = $KAFKA_TGT_CERT ibmcase/python37 bash -c \"python /home/PerfConsumer.py --topic source.accounts\" Or run the consumer inside openshift oc run kafka-consumer -ti --image=strimzi/kafka:latest-kafka-2.4.0 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server $KAFKA_TGT_BROKERS --topic source.accounts --from-beginning Start producer docker run -ti -v $( pwd ) :/home -e KAFKA_BROKERS = $KAFKA_SRC_BROKERS ibmcase/python37 bash -c \"python ProducerPerformance.py --file /home/ValidateTopicReplication/testplayload.json --size 500 --keyname identifier --topic accounts\" The number of records on the consumer side should match the number send (e.g. 500), but this time read from 5 partitions.","title":"Python based test approach"},{"location":"perf-tests/#conclusion-method-1","text":"Hypothesis proven - MirrorMaker 2 can replicate from a local topic with 10 partitions to a remote topic with 5 partitions.","title":"Conclusion Method 1"},{"location":"perf-tests/#java-based-performance-tests","text":"The performance testing in java is based on two tools: the IBM Event Stream tool which is itself based on the Apache Kafka producer performance tool and a custom consumer app, deployable on kubernetes cluster and that support latency reporting.","title":"Java based performance tests"},{"location":"perf-tests/#context","text":"The following diagram illustrates the performance test app context, with the two kafka clusters and the MirrorMaker tool: Zooming into the system, we define the producer and consumer apps and we may add metrics reporting consumable by using dashboard. For the app producer, we have cloned this tool under the perf-test/event-streams-sample-producer folder. The tool reports test metrics like records per second, number of records sent, the megabytes per second, the average and maximum latencies, from the producer.metrics() and other stats from the tools. The arguments supported are: Argument Description --record-size value > or --payload-file filename one is mandatory but not both. Work only for UTF-8 encoded text files --topic name mandatory --num-records value mandatory --payload-delimiter char delimiter to be used when --payload-file is provided. Default is \\n --throughput value Throttle to value messages/sec. -1 to disable throtlling --producer.config filename producer config properties file --producer-props prop_name=value producer configuration properties --print-metrics true|false Default to true --transactional-id value Test with transaction --transaction-duration-ms value The max age of each transaction. Test with transaction if v>0 To completement this tool, the consumer app is responsible to measure some of the latency between given timestamps. The following diagram presents the interesting time stamps we can assess with some tooling: ts-1: timestamp when creating the record object before sending ts-2: record timestamp when broker write to topic-partition: source topic ts-3: record timestamp when broker write to topic-partition: target topic ts-4: timestamp when polling the record To get timestamp at the topic level, we need to add the message.timestamp.type: LogAppendTime property when creating the topic. The consumer needs to be deployable on OpenShift to scale horizontally. The metrics can be exposed as metrics for Prometheus. The metrics are: average latency, min and max latencies. The performance test consumer webapp is under the perf-tests/perf-consumer-app folder . The readme, in this folder, explains how to build and deploy it.","title":"Context"},{"location":"perf-tests/#test-approach","text":"Using IBM event streams producer tool we can run 3 different workload size, the payload is generated with random bytes or with records read from a data file. java -jar target/es-producer.jar -t accounts -s small -c java -jar target/es-producer.jar -t accounts -s medium java -jar target/es-producer.jar -t accounts -s large To build the jar,run mvn package in the event-streams-sample-producer-1.1.0 folder. Here is an example of call to this performance producer java -jar event-streams-sample-producer-1.1.0/target/es-producer.jar --payload-file ./da/records.json -t topic --producer-config ../mirror-maker-2/eventstream.properties The eventstream.properties file define the bootstrap.servers, sasl configuration using Event Streams credentials and URLs. Something like: bootstrap.servers = broker-3-.....eventstreams.cloud.ibm.com:9093 security.protocol = SASL_SSL ssl.protocol = TLSv1.2 sasl.mechanism = PLAIN sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<replace with APIKAY>\"; The tool reports the following measures: 1261 records sent, 251 .8 records/sec ( 0 .97 MB/sec ) , 2233 .3 ms avg latency, 3320 .0 max latency. 2848 records sent, 569 .5 records/sec ( 2 .19 MB/sec ) , 5797 .0 ms avg latency, 8294 .0 max latency. 2840 records sent, 567 .8 records/sec ( 2 .19 MB/sec ) , 10760 .9 ms avg latency, 13288 .0 max latency. .. 60000 records sent, 490 .869821 records/sec ( 1 .89 MB/sec ) , 14698 .93 ms avg latency, 24949 .00 ms max latency, 14490 ms 50th, 22298 ms 95th, 23413 ms 99th, 24805 ms 99 .9th.","title":"Test approach"},{"location":"perf-tests/#measuring-with-consumer-app","text":"The consumer app offers an API to get performance metrics via HTTP or via metrics. The ones interesting are: /perf/config to get the cluster configuration, it should return a json document like { \"ssl.protocol\" : \"TLSv1.2\" , \"sasl.mechanism\" : \"PLAIN\" , \"key.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"client.id\" : \"test-cons-group-client-a425be84-801b-42b5-af2f-340386f98896\" , \"ssl.truststore.password\" : \"password\" , \"ssl.endpoint.identification.algorithm\" : \"HTTPS\" , \"ssl.enabled.protocols\" : \"TLSv1.2\" , \"ssl.truststore.location\" : \"/home/truststore.jks\" , \"bootstrap.servers\" : \"eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud:443\" , \"auto.offset.reset\" : \"earliest\" , \"value.deserializer\" : \"org.apache.kafka.common.serialization.StringDeserializer\" , \"group.id\" : \"test-cons-group\" , \"enable.auto.commit\" : \"false\" , \"security.protocol\" : \"SSL\" } /perf/current for getting the current metrics vias HTTP. Here is an example of output","title":"Measuring with consumer app"},{"location":"provisioning/","text":"Strimzi Operator and Kafka Cluster Provisioning In this note we propose to describe the provisioning of a Kafka Cluster using Strimzi operators and how to provision Mirror Maker 2 on Kubernetes or on VM. Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. The base of strimzi is to define a set of kubernetes operators and custom resource definitions for the different elements of Kafka. We recommend to go over the product overview page . The service account and role binding do not need to be re-installed if you did it previously. Concept summary The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics We address how to create topic using the operator in the section below . CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster, and are provided with Strimzi for each Kafka component used in a deployment. Strimzi Operators Deployment The Strimzi operator deployment is done in two phases: Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resources to deploy. Deploy one to many instances of those CRDs In CRD yaml file the kind attribute specifies the CRD to conform to. Each CRD has a common configuration like bootstrap servers, CPU resources, logging, healthchecks... If you want to deploy a Kafka Cluster using Strimzi on an OpenShift cluster use the steps in next sections. Create a namespace or openshift project kubectl create namespace eda-strimzi-kafka24 # Or using Openshift CLI oc new-project eda-strimzi-kafka24 Download the strimzi artefacts We have already created the configuration from the source strimzi github in the following folder openshift-strimzi/eda-strimzi-kafka24/cluster-operator . So you do not need to do the following steps if you use the same project name: eda-strimzi-kafka24 . In case you want to do on your own, get the last Strimzi release from this github page . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: eda-strimzi-kafka24 /' $strimzi -home/install/cluster-operator/*RoleBinding*.yaml Deploy the Custom Resource Definitions for kafka Custom resource definitions are defined within the kubernetes cluster. The following commands oc apply -f openshift-strimzi/eda-strimzi-kafka24/cluster-operator/ oc get crd In case of Strimzi cluster operator fails with error like: \" kafkas.kafka.strimzi.io is forbidden: User \"system:serviceaccount:eda-strimzi-kafka24 :strimzi-cluster-operator\" cannot watch resource \"kafkas\" in API group \"kafka.strimzi.io\" in the namespace \"eda-strimzi-kafka24 \", you need to add cluster role to the strimzi operator user by doing the following commands: oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 oc adm policy add-cluster-role-to-user strimzi-entity-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 The commands above, should create the following service account, resource definitions, roles, and role bindings: Names Resource Command strimzi-cluster-operator A service account provides an identity for processes that run in a Pod. oc get sa -l app=strimzi strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Roles oc get clusterrole strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding -l app=strimzi kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definitions oc get customresourcedefinition Note All those resources are labelled with strimzi name. Add Strimzi Admin Role If you want to allow non-kubernetes cluster administators to manage Strimzi resources, you must assign them to the Strimzi Administrator role. First deploy the role definition using the following command: oc apply -f openshift-strimzi/eda-strimzi-kafka24/010-ClusterRole-strimzi-admin.yaml Then assign the strimzi-admin ClusterRole to one or more existing users in the Kubernetes cluster. kubectl create clusterrolebinding strimzi-admin --clusterrole=strimzi-admin --user=<user-your-username-here> Deploy instances Deploy Kafka cluster The CRD for kafka cluster resource is here and we recommend to study it before defining your own cluster. Change the name of the cluster in one the yaml in the examples/kafka folder or use our openshift-strimzi/kafka-cluster.yml file in this project as a starting point. This file defines the default replication factor of 3 and in-synch replicas of 2. For development purpose we have set a plain (unencrypted) listener on port 9092 without TLS authentication. For external access to the kubernetes cluster, we need to have external listeners. As Openshift uses routes for external access, we need to add the external.type = route stanza in the yaml file. When exposing Kafka using OpenShift Routes and the HAProxy router, a dedicated Route is created for every Kafka broker pod. An additional Route is created to serve as a Kafka bootstrap address. Kafka clients can use the bootstrap route to connect to Kafka on port 443. Even for development we added the metrics rules in the metrics stamza within kafka-cluster.yml file to expose kafka and zookeeper metrics for Prometheus. For production we need to use persistence for the kafka log, ingress or load balancer external listener and rack awareness policies. It has to use Mutual TLS authentication, and with Strimzi we can use the User Operator to manage cluster users. Mutual authentication or two-way authentication is when both the server and the client present certificates. Using non presistence: oc apply -f openshift-strimzi/kafka-cluster.yaml oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 When looking at the pods running we can see the three kafka and zookeeper nodes as pods, the strimzi entity operator pod and the strimzi cluster operator. $ oc get pods my-cluster-entity-operator-645fdbc4cb-m29nk 3 /3 Running 0 18d my-cluster-kafka-0 2 /2 Running 0 3d my-cluster-kafka-1 2 /2 Running 0 3d my-cluster-kafka-2 2 /2 Running 0 3d my-cluster-zookeeper-0 2 /2 Running 0 3d my-cluster-zookeeper-1 2 /2 Running 0 3d my-cluster-zookeeper-2 2 /2 Running 0 3d strimzi-cluster-operator-58cbbcb7d-bcqhm 1 /1 Running 2 18d strimzi-topic-operator-564654cb86-nbt58 1 /1 Running 1 18d To use persistence add persistence volume and declare the PVC in the yaml file and then reapply: oc apply -f strimzi/kafka-cluster.yaml Add Topic CRDs and operator This step is optional. Topic operator helps to manage Kafka topics via yaml configuration and map the topics as kubernetes resources so a command like oc get kafkatopics returns the list of topics. The operator keeps the resources and the kafka topics in synch. This allows you to declare a KafkaTopic as part of your application\u2019s deployment using yaml file. To manage Kafka topics with operators, first modify the file 05-Deployment-strimzi-topic-operator.yaml to reflect your cluster name env : - name : STRIMZI_RESOURCE_LABELS value : \"strimzi.io/cluster=eda-demo-24-cluster\" - name : STRIMZI_KAFKA_BOOTSTRAP_SERVERS value : eda-demo-24-cluster-kafka-bootstrap:9092 - name : STRIMZI_ZOOKEEPER_CONNECT value : eda-demo-24-cluster-zookeeper-client:2181 and then deploy the topic-operator . This operation will fail if there is no Kafka Broker and Zookeeper available: oc apply -f openshift-strimzi/install/topic-operator oc adm policy add-cluster-role-to-user strimzi-topic-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 This will add the following resources: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition Create a topic Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : accounts labels : strimzi.io/cluster : eda-demo-24-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 message.timestamp.type : LogAppendTime oc apply -f test.yaml oc get kafkatopics This creates a topic test in your kafka cluster. Add User CRDs and operator This step is optional. To manage Kafka user with operators modify the file 05-Deployment-strimzi-user-operator.yaml to reflect your cluster name and then deploy the user-operator: oc apply -f openshift-strimzi/install/user-operator Test with producer and consumer pods Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts on your local computer, remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka custer definition yaml file includes the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes eda-demo-24-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/eda-demo-24-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt oc extract secret/eda-demo-24-cluster-clients-ca-cert --keys = ca.crt --to = - >> ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt The alias is used to access keystore entries (key and trusted certificate entries). Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud:443 --producer-property security.protocol = SSL --producer-property ssl.truststore.password = password --producer-property ssl.truststore.location = /home/truststore.jks --topic test Those properties can be in file bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Strimzi Kafka 2.4 Provisioning on Openshift"},{"location":"provisioning/#strimzi-operator-and-kafka-cluster-provisioning","text":"In this note we propose to describe the provisioning of a Kafka Cluster using Strimzi operators and how to provision Mirror Maker 2 on Kubernetes or on VM. Strimzi uses the Cluster Operator to deploy and manage Kafka (including Zookeeper) and Kafka Connect clusters. When the Strimzi Cluster Operator is up, it starts to watch for certain OpenShift or Kubernetes resources containing the desired Kafka and/or Kafka Connect cluster configuration. The base of strimzi is to define a set of kubernetes operators and custom resource definitions for the different elements of Kafka. We recommend to go over the product overview page . The service account and role binding do not need to be re-installed if you did it previously.","title":"Strimzi Operator and Kafka Cluster Provisioning"},{"location":"provisioning/#concept-summary","text":"The Cluster Operator is a pod used to deploys and manages Apache Kafka clusters, Kafka Connect, Kafka MirrorMaker (1 and 2), Kafka Bridge, Kafka Exporter, and the Entity Operator. When deployed the following commands goes to the Cluster operator: # Get the current cluster list oc get kafka # get the list of topic oc get kafkatopics We address how to create topic using the operator in the section below . CRDs act as configuration instructions to describe the custom resources in a Kubernetes cluster, and are provided with Strimzi for each Kafka component used in a deployment.","title":"Concept summary"},{"location":"provisioning/#strimzi-operators-deployment","text":"The Strimzi operator deployment is done in two phases: Deploy the Custom Resource Definitions (CRDs), which act as specifications of the custom resources to deploy. Deploy one to many instances of those CRDs In CRD yaml file the kind attribute specifies the CRD to conform to. Each CRD has a common configuration like bootstrap servers, CPU resources, logging, healthchecks... If you want to deploy a Kafka Cluster using Strimzi on an OpenShift cluster use the steps in next sections.","title":"Strimzi Operators Deployment"},{"location":"provisioning/#create-a-namespace-or-openshift-project","text":"kubectl create namespace eda-strimzi-kafka24 # Or using Openshift CLI oc new-project eda-strimzi-kafka24","title":"Create a namespace or openshift project"},{"location":"provisioning/#download-the-strimzi-artefacts","text":"We have already created the configuration from the source strimzi github in the following folder openshift-strimzi/eda-strimzi-kafka24/cluster-operator . So you do not need to do the following steps if you use the same project name: eda-strimzi-kafka24 . In case you want to do on your own, get the last Strimzi release from this github page . Then modify the Role binding yaml files with the namespace set in previous step. sed -i '' 's/namespace: .*/namespace: eda-strimzi-kafka24 /' $strimzi -home/install/cluster-operator/*RoleBinding*.yaml","title":"Download the strimzi artefacts"},{"location":"provisioning/#deploy-the-custom-resource-definitions-for-kafka","text":"Custom resource definitions are defined within the kubernetes cluster. The following commands oc apply -f openshift-strimzi/eda-strimzi-kafka24/cluster-operator/ oc get crd In case of Strimzi cluster operator fails with error like: \" kafkas.kafka.strimzi.io is forbidden: User \"system:serviceaccount:eda-strimzi-kafka24 :strimzi-cluster-operator\" cannot watch resource \"kafkas\" in API group \"kafka.strimzi.io\" in the namespace \"eda-strimzi-kafka24 \", you need to add cluster role to the strimzi operator user by doing the following commands: oc adm policy add-cluster-role-to-user strimzi-cluster-operator-namespaced --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 oc adm policy add-cluster-role-to-user strimzi-entity-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 The commands above, should create the following service account, resource definitions, roles, and role bindings: Names Resource Command strimzi-cluster-operator A service account provides an identity for processes that run in a Pod. oc get sa -l app=strimzi strimzi-cluster-operator-global, strimzi-cluster-operator-namespaced, strimzi-entity-operator, strimzi-kafka-broker, strimzi-topic-operator Cluster Roles oc get clusterrole strimzi-cluster-operator-entity-operator-delegation, strimzi-cluster-operator, strimzi-cluster-operator-topic-operator-delegation Role binding oc get rolebinding strimzi-cluster-operator, strimzi-cluster-operator-kafka-broker-delegation Cluster Role Binding oc get clusterrolebinding -l app=strimzi kafkabridges, kafkaconnectors, kafkaconnects, kafkamirrormaker2s kafka, kafkatopics, kafkausers Custom Resource Definitions oc get customresourcedefinition Note All those resources are labelled with strimzi name.","title":"Deploy the Custom Resource Definitions for kafka"},{"location":"provisioning/#add-strimzi-admin-role","text":"If you want to allow non-kubernetes cluster administators to manage Strimzi resources, you must assign them to the Strimzi Administrator role. First deploy the role definition using the following command: oc apply -f openshift-strimzi/eda-strimzi-kafka24/010-ClusterRole-strimzi-admin.yaml Then assign the strimzi-admin ClusterRole to one or more existing users in the Kubernetes cluster. kubectl create clusterrolebinding strimzi-admin --clusterrole=strimzi-admin --user=<user-your-username-here>","title":"Add Strimzi Admin Role"},{"location":"provisioning/#deploy-instances","text":"","title":"Deploy instances"},{"location":"provisioning/#deploy-kafka-cluster","text":"The CRD for kafka cluster resource is here and we recommend to study it before defining your own cluster. Change the name of the cluster in one the yaml in the examples/kafka folder or use our openshift-strimzi/kafka-cluster.yml file in this project as a starting point. This file defines the default replication factor of 3 and in-synch replicas of 2. For development purpose we have set a plain (unencrypted) listener on port 9092 without TLS authentication. For external access to the kubernetes cluster, we need to have external listeners. As Openshift uses routes for external access, we need to add the external.type = route stanza in the yaml file. When exposing Kafka using OpenShift Routes and the HAProxy router, a dedicated Route is created for every Kafka broker pod. An additional Route is created to serve as a Kafka bootstrap address. Kafka clients can use the bootstrap route to connect to Kafka on port 443. Even for development we added the metrics rules in the metrics stamza within kafka-cluster.yml file to expose kafka and zookeeper metrics for Prometheus. For production we need to use persistence for the kafka log, ingress or load balancer external listener and rack awareness policies. It has to use Mutual TLS authentication, and with Strimzi we can use the User Operator to manage cluster users. Mutual authentication or two-way authentication is when both the server and the client present certificates. Using non presistence: oc apply -f openshift-strimzi/kafka-cluster.yaml oc get kafka # NAME DESIRED KAFKA REPLICAS DESIRED ZK REPLICAS # my-cluster 3 3 When looking at the pods running we can see the three kafka and zookeeper nodes as pods, the strimzi entity operator pod and the strimzi cluster operator. $ oc get pods my-cluster-entity-operator-645fdbc4cb-m29nk 3 /3 Running 0 18d my-cluster-kafka-0 2 /2 Running 0 3d my-cluster-kafka-1 2 /2 Running 0 3d my-cluster-kafka-2 2 /2 Running 0 3d my-cluster-zookeeper-0 2 /2 Running 0 3d my-cluster-zookeeper-1 2 /2 Running 0 3d my-cluster-zookeeper-2 2 /2 Running 0 3d strimzi-cluster-operator-58cbbcb7d-bcqhm 1 /1 Running 2 18d strimzi-topic-operator-564654cb86-nbt58 1 /1 Running 1 18d To use persistence add persistence volume and declare the PVC in the yaml file and then reapply: oc apply -f strimzi/kafka-cluster.yaml","title":"Deploy Kafka cluster"},{"location":"provisioning/#add-topic-crds-and-operator","text":"This step is optional. Topic operator helps to manage Kafka topics via yaml configuration and map the topics as kubernetes resources so a command like oc get kafkatopics returns the list of topics. The operator keeps the resources and the kafka topics in synch. This allows you to declare a KafkaTopic as part of your application\u2019s deployment using yaml file. To manage Kafka topics with operators, first modify the file 05-Deployment-strimzi-topic-operator.yaml to reflect your cluster name env : - name : STRIMZI_RESOURCE_LABELS value : \"strimzi.io/cluster=eda-demo-24-cluster\" - name : STRIMZI_KAFKA_BOOTSTRAP_SERVERS value : eda-demo-24-cluster-kafka-bootstrap:9092 - name : STRIMZI_ZOOKEEPER_CONNECT value : eda-demo-24-cluster-zookeeper-client:2181 and then deploy the topic-operator . This operation will fail if there is no Kafka Broker and Zookeeper available: oc apply -f openshift-strimzi/install/topic-operator oc adm policy add-cluster-role-to-user strimzi-topic-operator --serviceaccount strimzi-cluster-operator -n eda-strimzi-kafka24 This will add the following resources: Names Resource Command strimzi-topic-operator Service account oc get sa strimzi-topic-operator Role binding oc get rolebinding kafkatopics Custom Resource Definition oc get customresourcedefinition","title":"Add Topic CRDs and operator"},{"location":"provisioning/#create-a-topic","text":"Edit a yaml file like the following: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaTopic metadata : name : accounts labels : strimzi.io/cluster : eda-demo-24-cluster spec : partitions : 1 replicas : 3 config : retention.ms : 7200000 segment.bytes : 1073741824 message.timestamp.type : LogAppendTime oc apply -f test.yaml oc get kafkatopics This creates a topic test in your kafka cluster.","title":"Create a topic"},{"location":"provisioning/#add-user-crds-and-operator","text":"This step is optional. To manage Kafka user with operators modify the file 05-Deployment-strimzi-user-operator.yaml to reflect your cluster name and then deploy the user-operator: oc apply -f openshift-strimzi/install/user-operator","title":"Add User CRDs and operator"},{"location":"provisioning/#test-with-producer-and-consumer-pods","text":"Use kafka-consumer and producer tools from Kafka distribution. Verify within Dockerhub under the Strimzi account to get the lastest image tag (below we use -2.4.0 tag). # Start a consumer on test topic oc run kafka-consumer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap:9092 --topic test --from-beginning # Start a text producer oc run kafka-producer -ti --image = strimzi/kafka:latest-kafka-2.4.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap:9092 --topic test # enter text If you want to use the strimzi kafka docker image to run the above scripts on your local computer, remotely connect to a kafka cluster you need multiple things to happen: Be sure the kafka custer definition yaml file includes the external route stamza: spec : kafka : version : 2.4.0 replicas : 3 listeners : plain : {} tls : {} external : type : route Get the host ip address from the Route resource oc get routes eda-demo-24-cluster-kafka-bootstrap -o = jsonpath = '{.status.ingress[0].host}{\"\\n\"}' Get the TLS certificate from the broker oc get secrets oc extract secret/eda-demo-24-cluster-cluster-ca-cert --keys = ca.crt --to = - > ca.crt oc extract secret/eda-demo-24-cluster-clients-ca-cert --keys = ca.crt --to = - >> ca.crt # transform it fo java truststore keytool -import -trustcacerts -alias root -file ca.crt -keystore truststore.jks -storepass password -noprompt The alias is used to access keystore entries (key and trusted certificate entries). Start the docker container by mounting the local folder with the truststore.jks to the /home docker run -ti -v $( pwd ) :/home strimzi/kafka:latest-kafka-2.4.0 bash # inside the container uses the consumer tool bash-4.2$ cd /opt/kafka/bin bash-4.2$ ./kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud:443 --consumer-property security.protocol = SSL --consumer-property ssl.truststore.password = password --consumer-property ssl.truststore.location = /home/truststore.jks --topic test --from-beginning For a producer the approach is the same but using the producer properties: ./kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24.gse-eda-demo-43-fa9ee67c9ab6a7791435450358e564cc-0000.us-east.containers.appdomain.cloud:443 --producer-property security.protocol = SSL --producer-property ssl.truststore.password = password --producer-property ssl.truststore.location = /home/truststore.jks --topic test Those properties can be in file bootstrap.servers = eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud security.protocol = SSL ssl.truststore.password = password ssl.truststore.location = /home/truststore.jks and then use the following parameters in the command line: ./kafka-console-producer.sh --broker-list eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --producer.config /home/strimzi.properties --topic test ./kafka-console-consumer.sh --bootstrap-server eda-demo-24-cluster-kafka-bootstrap-eda-strimzi-kafka24 .gse-eda-demos-fa9ee67c9ab6a7791435450358e564cc-0001.us-east.containers.appdomain.cloud:443 --topic test --consumer.config /home/strimzi.properties --from-beginning","title":"Test with producer and consumer pods"},{"location":"security/","text":"Security considerations Concepts The following diagram illustrates the security context in term of encryption and authentication between the different cluster types. Event Streams on premise, as part of Cloud Pak for integration, supports TLSv1.2 and CA certificates. For Event Streams on Cloud as managed service, or on premise, the authentication and authorization are done using Java Authentication and Authorization Service using generic user name: token and APIKEY as password as part of the Jaas configuration. Here is an example of properties setting. sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<event-streams-apikey>\"; When deployed in kubernetes those properties can be set in secret as config map and injected inside the consumer or producer pods. For mirror maker 2, the CRD definition supports the authentication stanza that can use secret, which will be used by the MM2 java code to define a JaaS configuration. authentication: passwordSecret: secretName: es-api-secret password: password username: token type: plain For other app connecting to event streams, the code need to set other SaaS parameters to encrypt via TLS and authenticate via SASL (the security protocol is SASL_SSL): properties . put ( CommonClientConfigs . SECURITY_PROTOCOL_CONFIG , \"SASL_SSL\" ); properties . put ( SaslConfigs . SASL_MECHANISM , \"PLAIN\" ); properties . put ( SaslConfigs . SASL_JAAS_CONFIG , \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\" + env . get ( \"KAFKA_APIKEY\" ) + \"\\\";\" ); Encryption The Clients to Broker communications are encrypted. Event Streams clients need to define the protocol using properties like: properties . put ( SslConfigs . SSL_PROTOCOL_CONFIG , \"TLSv1.2\" ); properties . put ( SslConfigs . SSL_ENABLED_PROTOCOLS_CONFIG , \"TLSv1.2\" ); With Java code, the CA root and client certiticates needs to be in the Truststore.jks file. Host name verification is enabled by default to avoid man in the middle attacks. The clients code will verify the server\u2019s fully qualified domain name (FQDN) against Common Name or Subject Alternative Name. Event streams security considerations The product documentation explains how to control access to Kafka / Event Streams resources like cluster, topic, consumer group and control transaction support. The access is controled via roles: reader, writer, manager. Access is done via Identity and access management service, where policies are defined for each potential resources. User and serviceId can be used and assigned to role. Each service policy defines the level of access that the service ID has to each resource or set of resources. A policy consists of the following information: The role assigned to the policy. For example, Viewer, Editor, or Operator. The type of service the policy applies to. For example, IBM Event Streams. The instance of the service to be secured. The type of resource to be secured. The valid values are cluster, topic, group, or txnid. Specifying a type is optional. If you do not specify a type, the policy then applies to all resources in the service instance. The identifier of the resource to be secured. Specify for resources of type topic, group and txnid. If you do not specify the resource, the policy then applies to all resources of the type specified in the service instance. For each topic it is possible to control the access type, and get a specific API key for accessing one or all the topics. ACL replications The following scenario validates the Mirror Maker 2 ACL replication. The goal is to define the same users between Event Streams on the cloud and local kafka may be deployed and operated with Strimzi. Two users are created (kp-consumer-1,kp-consumer-2) to access two different topics (kp-topic-1, kp-topic-2) respectively. Create new user, named kp-consumer-1 on local Kafka environment, using for example a Strimzi Kafka User, or using any mechanism to add user to k8s cluster. The following descriptor defines the ACL at a given topic: kp-topic-1 apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : kp-consumer-1 labels : strimzi.io/cluster : kp-local namespace : kp-kafka spec : authentication : type : tls authorization : type : simple acls : - resource : type : topic name : kp-topic-1 patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-1 patternType : literal operation : Describe host : '*' - resource : type : group name : kp-users patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-1 patternType : literal operation : Write host : '*' Do the same for a second user kp-consumer-2 accessing another topic `kp-topic-2: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : kp-consumer-2 labels : strimzi.io/cluster : kp-local namespace : kp-kafka spec : authentication : type : tls authorization : type : simple acls : - resource : type : topic name : kp-topic-2 patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-2 patternType : literal operation : Describe host : '*' - resource : type : group name : kp-users patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-2 patternType : literal operation : Write host : '*' On the IBM Event Streams side, define credentials for the different potential access roles: reader, writer or manager. Those credentials are automatically visible as Service IDs inside IBM Access Manager. Using IBM IAM and the service ID, define the access control for each of the service ID. The following figure In the mirror maker 2 configuration you can enable/disable the ACLs synch flag using the property: sync.topic.acls.enabled = true If it was set to false (default), we can assess the access per topic using the kafka ACL tool. User kp-consumer-1 has access to source topic kp-topic-1 and the remote topic on target cluster has no ACL moved. sh-4.2$ bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect = localhost:2181 --list --topic kp-topic-1 Current ACLs for resource ` Topic:LITERAL:kp-topic-1 ` : User:CN = kp-consumer-1 has Allow permission for operations: Write from hosts: * User:CN = kp-consumer-1 has Allow permission for operations: Describe from hosts: * User:CN = kp-consumer-1 has Allow permission for operations: Read from hosts: * sh-4.2$ bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect = localhost:2181 --list --topic kp-remote.kp-topic-1","title":"Security"},{"location":"security/#security-considerations","text":"","title":"Security considerations"},{"location":"security/#concepts","text":"The following diagram illustrates the security context in term of encryption and authentication between the different cluster types. Event Streams on premise, as part of Cloud Pak for integration, supports TLSv1.2 and CA certificates. For Event Streams on Cloud as managed service, or on premise, the authentication and authorization are done using Java Authentication and Authorization Service using generic user name: token and APIKEY as password as part of the Jaas configuration. Here is an example of properties setting. sasl.jaas.config = org.apache.kafka.common.security.plain.PlainLoginModule required username=\"token\" password=\"<event-streams-apikey>\"; When deployed in kubernetes those properties can be set in secret as config map and injected inside the consumer or producer pods. For mirror maker 2, the CRD definition supports the authentication stanza that can use secret, which will be used by the MM2 java code to define a JaaS configuration. authentication: passwordSecret: secretName: es-api-secret password: password username: token type: plain For other app connecting to event streams, the code need to set other SaaS parameters to encrypt via TLS and authenticate via SASL (the security protocol is SASL_SSL): properties . put ( CommonClientConfigs . SECURITY_PROTOCOL_CONFIG , \"SASL_SSL\" ); properties . put ( SaslConfigs . SASL_MECHANISM , \"PLAIN\" ); properties . put ( SaslConfigs . SASL_JAAS_CONFIG , \"org.apache.kafka.common.security.plain.PlainLoginModule required username=\\\"token\\\" password=\\\"\" + env . get ( \"KAFKA_APIKEY\" ) + \"\\\";\" );","title":"Concepts"},{"location":"security/#encryption","text":"The Clients to Broker communications are encrypted. Event Streams clients need to define the protocol using properties like: properties . put ( SslConfigs . SSL_PROTOCOL_CONFIG , \"TLSv1.2\" ); properties . put ( SslConfigs . SSL_ENABLED_PROTOCOLS_CONFIG , \"TLSv1.2\" ); With Java code, the CA root and client certiticates needs to be in the Truststore.jks file. Host name verification is enabled by default to avoid man in the middle attacks. The clients code will verify the server\u2019s fully qualified domain name (FQDN) against Common Name or Subject Alternative Name.","title":"Encryption"},{"location":"security/#event-streams-security-considerations","text":"The product documentation explains how to control access to Kafka / Event Streams resources like cluster, topic, consumer group and control transaction support. The access is controled via roles: reader, writer, manager. Access is done via Identity and access management service, where policies are defined for each potential resources. User and serviceId can be used and assigned to role. Each service policy defines the level of access that the service ID has to each resource or set of resources. A policy consists of the following information: The role assigned to the policy. For example, Viewer, Editor, or Operator. The type of service the policy applies to. For example, IBM Event Streams. The instance of the service to be secured. The type of resource to be secured. The valid values are cluster, topic, group, or txnid. Specifying a type is optional. If you do not specify a type, the policy then applies to all resources in the service instance. The identifier of the resource to be secured. Specify for resources of type topic, group and txnid. If you do not specify the resource, the policy then applies to all resources of the type specified in the service instance. For each topic it is possible to control the access type, and get a specific API key for accessing one or all the topics.","title":"Event streams security considerations"},{"location":"security/#acl-replications","text":"The following scenario validates the Mirror Maker 2 ACL replication. The goal is to define the same users between Event Streams on the cloud and local kafka may be deployed and operated with Strimzi. Two users are created (kp-consumer-1,kp-consumer-2) to access two different topics (kp-topic-1, kp-topic-2) respectively. Create new user, named kp-consumer-1 on local Kafka environment, using for example a Strimzi Kafka User, or using any mechanism to add user to k8s cluster. The following descriptor defines the ACL at a given topic: kp-topic-1 apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : kp-consumer-1 labels : strimzi.io/cluster : kp-local namespace : kp-kafka spec : authentication : type : tls authorization : type : simple acls : - resource : type : topic name : kp-topic-1 patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-1 patternType : literal operation : Describe host : '*' - resource : type : group name : kp-users patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-1 patternType : literal operation : Write host : '*' Do the same for a second user kp-consumer-2 accessing another topic `kp-topic-2: apiVersion : kafka.strimzi.io/v1beta1 kind : KafkaUser metadata : name : kp-consumer-2 labels : strimzi.io/cluster : kp-local namespace : kp-kafka spec : authentication : type : tls authorization : type : simple acls : - resource : type : topic name : kp-topic-2 patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-2 patternType : literal operation : Describe host : '*' - resource : type : group name : kp-users patternType : literal operation : Read host : '*' - resource : type : topic name : kp-topic-2 patternType : literal operation : Write host : '*' On the IBM Event Streams side, define credentials for the different potential access roles: reader, writer or manager. Those credentials are automatically visible as Service IDs inside IBM Access Manager. Using IBM IAM and the service ID, define the access control for each of the service ID. The following figure In the mirror maker 2 configuration you can enable/disable the ACLs synch flag using the property: sync.topic.acls.enabled = true If it was set to false (default), we can assess the access per topic using the kafka ACL tool. User kp-consumer-1 has access to source topic kp-topic-1 and the remote topic on target cluster has no ACL moved. sh-4.2$ bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect = localhost:2181 --list --topic kp-topic-1 Current ACLs for resource ` Topic:LITERAL:kp-topic-1 ` : User:CN = kp-consumer-1 has Allow permission for operations: Write from hosts: * User:CN = kp-consumer-1 has Allow permission for operations: Describe from hosts: * User:CN = kp-consumer-1 has Allow permission for operations: Read from hosts: * sh-4.2$ bin/kafka-acls.sh --authorizer kafka.security.auth.SimpleAclAuthorizer --authorizer-properties zookeeper.connect = localhost:2181 --list --topic kp-remote.kp-topic-1","title":"ACL replications"}]}